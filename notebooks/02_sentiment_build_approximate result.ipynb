{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2199429",
   "metadata": {},
   "source": [
    "# 02: Sentiment Analysis of News and Its Effect on Company’s Stock\n",
    "\n",
    "This notebook works with a single deduplicated news file produced in Notebook 01:\n",
    "\n",
    "- `outputs_01/news_deduped_2019_2023.csv`\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "1. Load the ticker universe from `outputs_01/selected_equities_only_2019_2023.csv`\n",
    "2. Run a column-completeness audit for the news file\n",
    "3. Evaluate multiple text variants with FinBERT and select the best one\n",
    "4. Build daily sentiment aggregates using 8 models\n",
    "5. Merge daily sentiment with daily price returns and build the master dataset\n",
    "6. Save all artifacts under `outputs_01/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d19205",
   "metadata": {},
   "source": [
    "## Блок 1: Импорты и базовые настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755adcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pandas.util import hash_pandas_object\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 180)\n",
    "pd.set_option(\"display.width\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6c8be",
   "metadata": {},
   "source": [
    "## Блок 2: Конфигурация проекта\n",
    "\n",
    "- период анализа (2019–2023),\n",
    "- пути к данным,\n",
    "- список тикеров (из ноутбука 01),\n",
    "- параметры чанков,\n",
    "- поведение чекпоинтов\n",
    "\n",
    "Записываем в `outputs_final/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e81bc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "OUTPUT_DIR: outputs_final\n",
      "CHECKPOINT_DIR: outputs_final/checkpoints/allnews\n"
     ]
    }
   ],
   "source": [
    "START_DATE = pd.Timestamp(\"2019-01-01\")\n",
    "END_DATE = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "OUTPUT_DIR = \"outputs_01\"\n",
    "\n",
    "NEWS_SOURCES = {\n",
    "    \"nasdaq\": os.path.join(OUTPUT_DIR, \"news_deduped_2019_2023.csv\"),\n",
    "}\n",
    "\n",
    "SELECTED_EQUITIES_PATH = os.path.join(OUTPUT_DIR, \"selected_equities_only_2019_2023.csv\")\n",
    "\n",
    "PRICES_DIR = \"full_history/full_history\"\n",
    "\n",
    "CHUNK_SIZE = 200000\n",
    "MAX_NEWS_PER_DAY = None\n",
    "\n",
    "RESUME_FROM_CHECKPOINTS = True\n",
    "FORCE_RECOMPUTE_CHECKPOINTS = False\n",
    "\n",
    "SKIP_IF_DAILY_EXISTS = True\n",
    "FORCE_REBUILD_DAILY = False\n",
    "\n",
    "RESET_CHECKPOINT_DIR = False\n",
    "\n",
    "RUN_TAG = \"allnews\" if (MAX_NEWS_PER_DAY is None or int(MAX_NEWS_PER_DAY) <= 0) else f\"max{int(MAX_NEWS_PER_DAY)}\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\", RUN_TAG)\n",
    "if RESET_CHECKPOINT_DIR and os.path.exists(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(CHECKPOINT_DIR)\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\"CHECKPOINT_DIR:\", CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288eaf5c",
   "metadata": {},
   "source": [
    "## Блок 3: Проверка входных файлов и загрузка списка тикеров\n",
    "\n",
    "Проверка наличия\n",
    "- `selected_equities_only_2019_2023.csv`,\n",
    "- CSV с новостями,\n",
    "- папки с ценами\n",
    "\n",
    "Дальше загружаем тикеры и собираем `TICKER_SET`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f1ec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тикеров в выборке: 2193\n",
      "Колонки в companies: ['ticker', 'company_name', 'sector', 'industry', 'exchange', 'quoteType', 'price_rows', 'price_min_date', 'price_max_date', 'coverage_pct', 'starts_in_2019', 'ends_in_2023', 'full_coverage', 'news_count', 'news_min_date', 'news_max_date']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>company_name</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>exchange</th>\n",
       "      <th>quoteType</th>\n",
       "      <th>price_rows</th>\n",
       "      <th>price_min_date</th>\n",
       "      <th>price_max_date</th>\n",
       "      <th>coverage_pct</th>\n",
       "      <th>starts_in_2019</th>\n",
       "      <th>ends_in_2023</th>\n",
       "      <th>full_coverage</th>\n",
       "      <th>news_count</th>\n",
       "      <th>news_min_date</th>\n",
       "      <th>news_max_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>Auto Manufacturers</td>\n",
       "      <td>NMS</td>\n",
       "      <td>EQUITY</td>\n",
       "      <td>1257</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>96.4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>12462</td>\n",
       "      <td>2019-07-01 00:00:00</td>\n",
       "      <td>2023-12-16 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INTC</td>\n",
       "      <td>Intel Corporation</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>NMS</td>\n",
       "      <td>EQUITY</td>\n",
       "      <td>1257</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>96.4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>11178</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>2023-12-16 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>Alphabet Inc.</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Internet Content &amp; Information</td>\n",
       "      <td>NMS</td>\n",
       "      <td>EQUITY</td>\n",
       "      <td>1257</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>96.4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>11040</td>\n",
       "      <td>2019-01-02 00:00:00</td>\n",
       "      <td>2023-12-16 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DIS</td>\n",
       "      <td>The Walt Disney Company</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>NYQ</td>\n",
       "      <td>EQUITY</td>\n",
       "      <td>1257</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>96.4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10597</td>\n",
       "      <td>2019-06-13 00:00:00</td>\n",
       "      <td>2023-12-16 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Consumer Electronics</td>\n",
       "      <td>NMS</td>\n",
       "      <td>EQUITY</td>\n",
       "      <td>1257</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>96.4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>9811</td>\n",
       "      <td>2020-03-09 00:00:00</td>\n",
       "      <td>2023-12-16 22:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker             company_name                  sector                        industry exchange quoteType  price_rows price_min_date price_max_date  coverage_pct  starts_in_2019  ends_in_2023  \\\n",
       "0   TSLA              Tesla, Inc.       Consumer Cyclical              Auto Manufacturers      NMS    EQUITY        1257     2019-01-02     2023-12-28          96.4            True          True   \n",
       "1   INTC        Intel Corporation              Technology                  Semiconductors      NMS    EQUITY        1257     2019-01-02     2023-12-28          96.4            True          True   \n",
       "2   GOOG            Alphabet Inc.  Communication Services  Internet Content & Information      NMS    EQUITY        1257     2019-01-02     2023-12-28          96.4            True          True   \n",
       "3    DIS  The Walt Disney Company  Communication Services                   Entertainment      NYQ    EQUITY        1257     2019-01-02     2023-12-28          96.4            True          True   \n",
       "4   AAPL               Apple Inc.              Technology            Consumer Electronics      NMS    EQUITY        1257     2019-01-02     2023-12-28          96.4            True          True   \n",
       "\n",
       "   full_coverage  news_count        news_min_date        news_max_date  \n",
       "0           True       12462  2019-07-01 00:00:00  2023-12-16 22:00:00  \n",
       "1           True       11178  2019-01-01 00:00:00  2023-12-16 10:00:00  \n",
       "2           True       11040  2019-01-02 00:00:00  2023-12-16 23:00:00  \n",
       "3           True       10597  2019-06-13 00:00:00  2023-12-16 19:00:00  \n",
       "4           True        9811  2020-03-09 00:00:00  2023-12-16 22:00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(SELECTED_EQUITIES_PATH):\n",
    "    raise FileNotFoundError(f\"Ticker file not found: {SELECTED_EQUITIES_PATH}\")\n",
    "\n",
    "missing_news = [fp for fp in NEWS_SOURCES.values() if not os.path.exists(fp)]\n",
    "if missing_news:\n",
    "    raise FileNotFoundError(f\"News file(s) not found: {missing_news}\")\n",
    "\n",
    "if not os.path.isdir(PRICES_DIR):\n",
    "    raise FileNotFoundError(f\"Prices directory not found: {PRICES_DIR}\")\n",
    "\n",
    "companies = pd.read_csv(SELECTED_EQUITIES_PATH, low_memory=False)\n",
    "companies[\"ticker\"] = companies[\"ticker\"].astype(str).str.strip()\n",
    "TICKER_SET = set(companies[\"ticker\"].tolist())\n",
    "\n",
    "print(\"Tickers in universe:\", len(TICKER_SET))\n",
    "display(companies.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8e477",
   "metadata": {},
   "source": [
    "## Блок 4: Нормализация и фильтрация новостей (общий препроцессинг)\n",
    "\n",
    "Цель: привести два датасета новостей к общей схеме и сразу отфильтровать все нерелевантное\n",
    "\n",
    "Ключевые правила:\n",
    "- оставляем только строки, где заполнен `Stock_symbol` (это привязка к тикеру),\n",
    "- парсим `Date` в datetime и строим календарный `day`,\n",
    "- фильтруем по `START_DATE - END_DATE`,\n",
    "- фильтруем по тикерам из `TICKER_SET`,\n",
    "- требуем непустой `Url` (он нужен для дедупликации),\n",
    "- убираем дубли внутри чанка по `(ticker, day, url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc120c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_COLS_RAW = [\n",
    "    \"Date\", \"Article_title\", \"Stock_symbol\", \"Url\", \"Publisher\", \"Author\",\n",
    "    \"Article\", \"Lsa_summary\", \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\",\n",
    "]\n",
    "\n",
    "def print_step(title: str):\n",
    "    line = \"=\" * 88\n",
    "    print(\"\\n\" + line)\n",
    "    print(title)\n",
    "    print(line)\n",
    "\n",
    "def _clean_str(s: pd.Series) -> pd.Series:\n",
    "    return s.fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "def _normalize_news_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ren = {}\n",
    "    if \"Date\" in df.columns and \"date\" not in df.columns:\n",
    "        ren[\"Date\"] = \"date\"\n",
    "    if \"Stock_symbol\" in df.columns and \"ticker\" not in df.columns:\n",
    "        ren[\"Stock_symbol\"] = \"ticker\"\n",
    "    if \"Url\" in df.columns and \"url\" not in df.columns:\n",
    "        ren[\"Url\"] = \"url\"\n",
    "    if \"Article_title\" in df.columns and \"title\" not in df.columns:\n",
    "        ren[\"Article_title\"] = \"title\"\n",
    "    if \"Article\" in df.columns and \"article\" not in df.columns:\n",
    "        ren[\"Article\"] = \"article\"\n",
    "    if \"Lsa_summary\" in df.columns and \"lsa_summary\" not in df.columns:\n",
    "        ren[\"Lsa_summary\"] = \"lsa_summary\"\n",
    "    if \"Luhn_summary\" in df.columns and \"luhn_summary\" not in df.columns:\n",
    "        ren[\"Luhn_summary\"] = \"luhn_summary\"\n",
    "    if \"Textrank_summary\" in df.columns and \"textrank_summary\" not in df.columns:\n",
    "        ren[\"Textrank_summary\"] = \"textrank_summary\"\n",
    "    if \"Lexrank_summary\" in df.columns and \"lexrank_summary\" not in df.columns:\n",
    "        ren[\"Lexrank_summary\"] = \"lexrank_summary\"\n",
    "    if \"Publisher\" in df.columns and \"publisher\" not in df.columns:\n",
    "        ren[\"Publisher\"] = \"publisher\"\n",
    "    if \"Author\" in df.columns and \"author\" not in df.columns:\n",
    "        ren[\"Author\"] = \"author\"\n",
    "    return df.rename(columns=ren) if ren else df\n",
    "\n",
    "def _ensure_cols(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    return df\n",
    "\n",
    "def _prep_news_chunk_base(raw: pd.DataFrame, source_tag: str) -> pd.DataFrame:\n",
    "    ch = _normalize_news_cols(raw)\n",
    "\n",
    "    need = [\n",
    "        \"date\", \"ticker\", \"url\", \"title\", \"article\",\n",
    "        \"lsa_summary\", \"luhn_summary\", \"textrank_summary\", \"lexrank_summary\",\n",
    "        \"publisher\", \"author\"\n",
    "    ]\n",
    "    ch = _ensure_cols(ch, need)\n",
    "\n",
    "    ch[\"ticker\"] = _clean_str(ch[\"ticker\"])\n",
    "    ch[\"url\"] = _clean_str(ch[\"url\"])\n",
    "\n",
    "    ch = ch[ch[\"ticker\"] != \"\"]\n",
    "    if ch.empty:\n",
    "        return ch.iloc[0:0].copy()\n",
    "\n",
    "    ch[\"date\"] = pd.to_datetime(ch[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "    ch = ch.dropna(subset=[\"date\"])\n",
    "    if ch.empty:\n",
    "        return ch.iloc[0:0].copy()\n",
    "\n",
    "    ch[\"day\"] = ch[\"date\"].dt.floor(\"D\")\n",
    "    ch = ch[(ch[\"day\"] >= START_DATE) & (ch[\"day\"] <= END_DATE)]\n",
    "    if ch.empty:\n",
    "        return ch.iloc[0:0].copy()\n",
    "\n",
    "    ch = ch[ch[\"ticker\"].isin(TICKER_SET)]\n",
    "    if ch.empty:\n",
    "        return ch.iloc[0:0].copy()\n",
    "\n",
    "    if MAX_NEWS_PER_DAY is not None and int(MAX_NEWS_PER_DAY) > 0:\n",
    "        ch = (\n",
    "            ch.sort_values([\"ticker\", \"day\"], kind=\"mergesort\")\n",
    "              .groupby([\"ticker\", \"day\"], as_index=False, sort=False)\n",
    "              .head(int(MAX_NEWS_PER_DAY))\n",
    "        )\n",
    "\n",
    "    url = _clean_str(ch[\"url\"]).str.lower()\n",
    "    title = _clean_str(ch[\"title\"]).str.lower()\n",
    "    day_s = ch[\"day\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    ticker = ch[\"ticker\"].astype(str)\n",
    "\n",
    "    key = np.where(url != \"\", (url + \"||\" + ticker).values, (day_s + \"||\" + title + \"||\" + ticker).values)\n",
    "    h = hash_pandas_object(pd.Series(key), index=False).astype(\"uint64\")\n",
    "    ch = ch.loc[~h.duplicated()].copy()\n",
    "\n",
    "    ch[\"source\"] = source_tag\n",
    "\n",
    "    keep_cols = [\n",
    "        \"ticker\", \"day\", \"url\",\n",
    "        \"title\", \"article\",\n",
    "        \"lsa_summary\", \"luhn_summary\", \"textrank_summary\", \"lexrank_summary\",\n",
    "        \"publisher\", \"author\",\n",
    "        \"source\"\n",
    "    ]\n",
    "    return ch[keep_cols].reset_index(drop=True)\n",
    "\n",
    "def build_text_variant(df: pd.DataFrame, variant: str) -> pd.Series:\n",
    "    title = _clean_str(df[\"title\"]) if \"title\" in df.columns else pd.Series([\"\"] * len(df), index=df.index)\n",
    "    article = _clean_str(df[\"article\"]) if \"article\" in df.columns else pd.Series([\"\"] * len(df), index=df.index)\n",
    "\n",
    "    lsa = _clean_str(df[\"lsa_summary\"]) if \"lsa_summary\" in df.columns else pd.Series([\"\"] * len(df), index=df.index)\n",
    "    luhn = _clean_str(df[\"luhn_summary\"]) if \"luhn_summary\" in df.columns else pd.Series([\"\"] * len(df), index=df.index)\n",
    "    tr = _clean_str(df[\"textrank_summary\"]) if \"textrank_summary\" in df.columns else pd.Series([\"\"] * len(df), index=df.index)\n",
    "    lex = _clean_str(df[\"lexrank_summary\"]) if \"lexrank_summary\" in df.columns else pd.Series([\"\"] * len(df), index=df.index)\n",
    "\n",
    "    if variant == \"title_article\":\n",
    "        comb = (title + \". \" + article).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "        out = pd.Series([\"\"] * len(df), index=df.index, dtype=\"string\")\n",
    "        both = (title != \"\") & (article != \"\")\n",
    "        only_t = (title != \"\") & (article == \"\")\n",
    "        only_a = (title == \"\") & (article != \"\")\n",
    "        out.loc[both] = comb.loc[both]\n",
    "        out.loc[only_t] = title.loc[only_t]\n",
    "        out.loc[only_a] = article.loc[only_a]\n",
    "        return out.astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    if variant == \"lsa_summary\":\n",
    "        return lsa.where(lsa != \"\", \"\")\n",
    "    if variant == \"luhn_summary\":\n",
    "        return luhn.where(luhn != \"\", \"\")\n",
    "    if variant == \"textrank_summary\":\n",
    "        return tr.where(tr != \"\", \"\")\n",
    "    if variant == \"lexrank_summary\":\n",
    "        return lex.where(lex != \"\", \"\")\n",
    "\n",
    "    raise ValueError(f\"Unknown text variant: {variant}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6b492",
   "metadata": {},
   "source": [
    "## Блок 5: Аудит заполненности колонок для каждого датасета\n",
    "\n",
    "Мы считаем `NaN` **в двух режимах**:\n",
    "1) по всем строкам файла\n",
    "2) **после фильтрации**: только строки с `Stock_symbol`, тикером из `TICKER_SET`, датой 2019–2023 и непустым `Url`\n",
    "\n",
    "Также пытаемся вывести **5 строк**, где заполнены **все интересующие нас столбцы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a1a5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: all_external | FILE: All_external.csv\n",
      "Size (MB): 5465.9\n",
      "Usecols: ['Date', 'Stock_symbol', 'Article_title', 'Url', 'Publisher', 'Author', 'Lsa_summary', 'Luhn_summary', 'Textrank_summary', 'Lexrank_summary']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "audit all_external: 66it [00:34,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows total: 13,057,514\n",
      "Rows after filters: 335,004 (2.57%)\n",
      "\n",
      "NaN summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>nan_total</th>\n",
       "      <th>nan_total_pct</th>\n",
       "      <th>nan_after_filters</th>\n",
       "      <th>nan_after_filters_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Author</td>\n",
       "      <td>11871199</td>\n",
       "      <td>90.91</td>\n",
       "      <td>335004</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lsa_summary</td>\n",
       "      <td>13057514</td>\n",
       "      <td>100.00</td>\n",
       "      <td>335004</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Luhn_summary</td>\n",
       "      <td>13057514</td>\n",
       "      <td>100.00</td>\n",
       "      <td>335004</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Textrank_summary</td>\n",
       "      <td>13057514</td>\n",
       "      <td>100.00</td>\n",
       "      <td>335004</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lexrank_summary</td>\n",
       "      <td>13057514</td>\n",
       "      <td>100.00</td>\n",
       "      <td>335004</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Date</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stock_symbol</td>\n",
       "      <td>9804627</td>\n",
       "      <td>75.09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article_title</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Url</td>\n",
       "      <td>686</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Publisher</td>\n",
       "      <td>9030871</td>\n",
       "      <td>69.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             column  nan_total  nan_total_pct  nan_after_filters  nan_after_filters_pct\n",
       "5            Author   11871199          90.91             335004                  100.0\n",
       "6       Lsa_summary   13057514         100.00             335004                  100.0\n",
       "7      Luhn_summary   13057514         100.00             335004                  100.0\n",
       "8  Textrank_summary   13057514         100.00             335004                  100.0\n",
       "9   Lexrank_summary   13057514         100.00             335004                  100.0\n",
       "0              Date          0           0.00                  0                    0.0\n",
       "1      Stock_symbol    9804627          75.09                  0                    0.0\n",
       "2     Article_title          1           0.00                  0                    0.0\n",
       "3               Url        686           0.01                  0                    0.0\n",
       "4         Publisher    9030871          69.16                  0                    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples (5 rows) with ALL columns filled:\n",
      "Не найдено строк, где заполнены ВСЕ указанные столбцы\n",
      "SOURCE: nasdaq | FILE: nasdaq_exteral_data.csv\n",
      "Size (MB): 22156.7\n",
      "Usecols: ['Date', 'Stock_symbol', 'Article_title', 'Url', 'Publisher', 'Author', 'Lsa_summary', 'Luhn_summary', 'Textrank_summary', 'Lexrank_summary']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "audit nasdaq: 78it [01:39,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows total: 15,549,299\n",
      "Rows after filters: 1,253,430 (8.06%)\n",
      "\n",
      "NaN summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>nan_total</th>\n",
       "      <th>nan_total_pct</th>\n",
       "      <th>nan_after_filters</th>\n",
       "      <th>nan_after_filters_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Author</td>\n",
       "      <td>14362984</td>\n",
       "      <td>92.37</td>\n",
       "      <td>1253430</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Publisher</td>\n",
       "      <td>11522656</td>\n",
       "      <td>74.10</td>\n",
       "      <td>918426</td>\n",
       "      <td>73.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lsa_summary</td>\n",
       "      <td>13057522</td>\n",
       "      <td>83.97</td>\n",
       "      <td>335005</td>\n",
       "      <td>26.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Luhn_summary</td>\n",
       "      <td>13057521</td>\n",
       "      <td>83.97</td>\n",
       "      <td>335004</td>\n",
       "      <td>26.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Textrank_summary</td>\n",
       "      <td>13057521</td>\n",
       "      <td>83.97</td>\n",
       "      <td>335004</td>\n",
       "      <td>26.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lexrank_summary</td>\n",
       "      <td>13057521</td>\n",
       "      <td>83.97</td>\n",
       "      <td>335004</td>\n",
       "      <td>26.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Date</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stock_symbol</td>\n",
       "      <td>9804627</td>\n",
       "      <td>63.06</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article_title</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Url</td>\n",
       "      <td>686</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             column  nan_total  nan_total_pct  nan_after_filters  nan_after_filters_pct\n",
       "5            Author   14362984          92.37            1253430                 100.00\n",
       "4         Publisher   11522656          74.10             918426                  73.27\n",
       "6       Lsa_summary   13057522          83.97             335005                  26.73\n",
       "7      Luhn_summary   13057521          83.97             335004                  26.73\n",
       "8  Textrank_summary   13057521          83.97             335004                  26.73\n",
       "9   Lexrank_summary   13057521          83.97             335004                  26.73\n",
       "0              Date          0           0.00                  0                   0.00\n",
       "1      Stock_symbol    9804627          63.06                  0                   0.00\n",
       "2     Article_title          1           0.00                  0                   0.00\n",
       "3               Url        686           0.00                  0                   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples (5 rows) with ALL columns filled:\n",
      "Не найдено строк, где заполнены ВСЕ указанные столбцы\n"
     ]
    }
   ],
   "source": [
    "AUDIT_COLS = [\n",
    "    \"Date\", \"Stock_symbol\", \"Article_title\", \"Url\", \"Publisher\", \"Author\",\n",
    "    \"Lsa_summary\", \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\"\n",
    "]\n",
    "\n",
    "def audit_nan_counts(file_path: str, source_tag: str, max_chunks: Optional[int] = None) -> pd.DataFrame:\n",
    "    header = pd.read_csv(file_path, nrows=0, low_memory=False)\n",
    "    usecols = [c for c in AUDIT_COLS if c in header.columns]\n",
    "    missing = [c for c in AUDIT_COLS if c not in header.columns]\n",
    "\n",
    "    print(f\"SOURCE: {source_tag} | FILE: {file_path}\")\n",
    "    print(\"Size (MB):\", round(os.path.getsize(file_path)/1024/1024, 1))\n",
    "    print(\"Usecols:\", usecols)\n",
    "    if missing:\n",
    "        print(\"Missing columns in this file:\", missing)\n",
    "\n",
    "    total = 0\n",
    "    nan_total = pd.Series(0, index=usecols, dtype=\"int64\")\n",
    "\n",
    "    total_f = 0\n",
    "    nan_f = pd.Series(0, index=usecols, dtype=\"int64\")\n",
    "\n",
    "    examples_full = []\n",
    "    examples_full_needed = 5\n",
    "\n",
    "    reader = pd.read_csv(file_path, usecols=usecols, chunksize=CHUNK_SIZE, low_memory=False)\n",
    "\n",
    "    for i, raw in enumerate(tqdm(reader, desc=f\"audit {source_tag}\")):\n",
    "        if (max_chunks is not None) and (i >= int(max_chunks)):\n",
    "            break\n",
    "\n",
    "        total += len(raw)\n",
    "        nan_total += raw[usecols].isna().sum().astype(\"int64\")\n",
    "\n",
    "        ch = _normalize_news_cols(raw)\n",
    "        ch = _ensure_cols(ch, [\"date\",\"ticker\",\"url\",\"title\",\"publisher\",\"author\",\n",
    "                               \"lsa_summary\",\"luhn_summary\",\"textrank_summary\",\"lexrank_summary\"])\n",
    "\n",
    "        ch[\"ticker\"] = _clean_str(ch[\"ticker\"])\n",
    "        ch[\"url\"] = _clean_str(ch[\"url\"])\n",
    "        ch = ch[ch[\"ticker\"] != \"\"]\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        ch[\"date\"] = pd.to_datetime(ch[\"date\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
    "        ch = ch.dropna(subset=[\"date\"])\n",
    "        if ch.empty:\n",
    "            continue\n",
    "        ch[\"day\"] = ch[\"date\"].dt.floor(\"D\")\n",
    "        ch = ch[(ch[\"day\"] >= START_DATE) & (ch[\"day\"] <= END_DATE)]\n",
    "        ch = ch[ch[\"ticker\"].isin(TICKER_SET)]\n",
    "        ch = ch[ch[\"url\"] != \"\"]\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        total_f += len(ch)\n",
    "\n",
    "        ch_view = ch.copy()\n",
    "        remap = {\n",
    "            \"date\": \"Date\", \"ticker\": \"Stock_symbol\", \"url\": \"Url\", \"title\": \"Article_title\",\n",
    "            \"publisher\": \"Publisher\", \"author\": \"Author\",\n",
    "            \"lsa_summary\": \"Lsa_summary\", \"luhn_summary\": \"Luhn_summary\",\n",
    "            \"textrank_summary\": \"Textrank_summary\", \"lexrank_summary\": \"Lexrank_summary\",\n",
    "        }\n",
    "        inv = {v:k for k,v in remap.items()}\n",
    "\n",
    "        for c in usecols:\n",
    "            nc = inv.get(c)\n",
    "            if (nc is not None) and (nc in ch_view.columns):\n",
    "                nan_f[c] += ch_view[nc].isna().sum()\n",
    "            else:\n",
    "                nan_f[c] += len(ch_view)\n",
    "\n",
    "        if len(examples_full) < examples_full_needed:\n",
    "            tmp = ch_view.copy()\n",
    "            ex = pd.DataFrame(index=tmp.index)\n",
    "            for c in usecols:\n",
    "                nc = inv.get(c)\n",
    "                if (nc is not None) and (nc in tmp.columns):\n",
    "                    ex[c] = tmp[nc]\n",
    "                else:\n",
    "                    ex[c] = np.nan\n",
    "\n",
    "            m = pd.Series(True, index=ex.index)\n",
    "            for c in usecols:\n",
    "                m &= ex[c].notna() & (_clean_str(ex[c]) != \"\")\n",
    "            full_rows = ex.loc[m].head(examples_full_needed - len(examples_full))\n",
    "            if not full_rows.empty:\n",
    "                examples_full.extend(full_rows.to_dict(orient=\"records\"))\n",
    "\n",
    "    out = pd.DataFrame({\"column\": usecols})\n",
    "    out[\"nan_total\"] = out[\"column\"].map(nan_total).astype(\"int64\")\n",
    "    out[\"nan_total_pct\"] = (out[\"nan_total\"] / max(total,1) * 100).round(2)\n",
    "\n",
    "    out[\"nan_after_filters\"] = out[\"column\"].map(nan_f).astype(\"int64\")\n",
    "    out[\"nan_after_filters_pct\"] = (out[\"nan_after_filters\"] / max(total_f,1) * 100).round(2)\n",
    "\n",
    "    print(\"Rows total:\", f\"{total:,}\")\n",
    "    print(\"Rows after filters:\", f\"{total_f:,}\", f\"({(total_f/max(total,1)*100):.2f}%)\")\n",
    "\n",
    "    print(\"\\nNaN summary:\")\n",
    "    display(out.sort_values(\"nan_after_filters_pct\", ascending=False))\n",
    "\n",
    "    print(\"\\nExamples (5 rows) with ALL columns filled:\")\n",
    "    if examples_full:\n",
    "        display(pd.DataFrame(examples_full)[usecols])\n",
    "    else:\n",
    "        print(\"Не найдено строк, где заполнены ВСЕ указанные столбцы\")\n",
    "\n",
    "    return out\n",
    "\n",
    "audit_tables = {}\n",
    "for tag, fp in NEWS_SOURCES.items():\n",
    "    audit_tables[tag] = audit_nan_counts(fp, tag, max_chunks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f3289",
   "metadata": {},
   "source": [
    "## Блок 6: Проверка дубликатов URL между источниками\n",
    "\n",
    "Один и тот же материал может оказаться в обоих CSV -> если считать оба, мы **удвоим** вес новости\n",
    "\n",
    "Что делаем:\n",
    "1. Считаем пересечение URL (после фильтра тикеров/дат/Stock_symbol)\n",
    "2. Оцениваем полноту источников: среднее число заполненных **текстовых** полей на строку (`Article_title`, `Article`, summaries)\n",
    "3. Выбираем **предпочтительный источник** для дубликатов: если URL встречается в обоих, берем строку из более полного источника\n",
    "4. Строим список ключей дедупликации `key = hash(url || ticker)` для предпочтительного источника и сохраняем: `outputs_final/dedup/preferred_keys__<source>.npy`\n",
    "\n",
    "Дальше этот список будет использоваться, чтобы **пропускать дубликаты** во втором источнике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9312844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "Скан источников для оценки полноты и пересечения URL\n",
      "========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scan all_external: 66it [00:45,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_external rows_filtered: 333,251 | unique_url: 207,042 | mean_filled_text_fields: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scan nasdaq: 78it [02:24,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasdaq rows_filtered: 1,251,664 | unique_url: 761,320 | mean_filled_text_fields: 4.669\n",
      "\n",
      "URL overlap (url-only): 207,042\n",
      "URL overlap (url,ticker): 333,177\n",
      "\n",
      "Preferred source for duplicates: nasdaq\n",
      "Other source: all_external\n",
      "Saved preferred keys: outputs_final/dedup/preferred_keys__nasdaq.npy | count: 1251590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "606"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEDUP_DIR = os.path.join(OUTPUT_DIR, \"dedup\")\n",
    "os.makedirs(DEDUP_DIR, exist_ok=True)\n",
    "\n",
    "preferred_source = list(NEWS_SOURCES.keys())[0]\n",
    "preferred_keys_sorted = None\n",
    "\n",
    "def _hash_url_ticker(df: pd.DataFrame) -> np.ndarray:\n",
    "    if \"url\" not in df.columns or \"ticker\" not in df.columns:\n",
    "        return np.zeros(len(df), dtype=\"uint64\")\n",
    "    key = _clean_str(df[\"url\"]) + \"||\" + _clean_str(df[\"ticker\"])\n",
    "    return hash_pandas_object(key, index=False).astype(\"uint64\").to_numpy()\n",
    "\n",
    "def _hash_url_only(df: pd.DataFrame) -> np.ndarray:\n",
    "    if \"url\" not in df.columns:\n",
    "        return np.zeros(len(df), dtype=\"uint64\")\n",
    "    key = _clean_str(df[\"url\"])\n",
    "    return hash_pandas_object(key, index=False).astype(\"uint64\").to_numpy()\n",
    "\n",
    "def _is_dup_by_keys(keys_sorted: np.ndarray, keys: np.ndarray) -> np.ndarray:\n",
    "    if keys_sorted is None or len(keys_sorted) == 0:\n",
    "        return np.zeros(len(keys), dtype=bool)\n",
    "    idx = np.searchsorted(keys_sorted, keys)\n",
    "    return (idx < len(keys_sorted)) & (keys_sorted[idx] == keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2bd6d3",
   "metadata": {},
   "source": [
    "## Блок 7: Сентимент по разным вариантам текста\n",
    "\n",
    "### Кандидаты текста (5 вариантов)\n",
    "- `title_article` = `Article_title + Article`\n",
    "- `lsa_summary`\n",
    "- `luhn_summary`\n",
    "- `textrank_summary`\n",
    "- `lexrank_summary`\n",
    "\n",
    "Если для конкретного варианта поле отсутствует/пустое -> строка **не участвует** для этого варианта\n",
    "\n",
    "Для каждого источника и варианта:\n",
    "- покрытие (`coverage`) = доля строк после фильтров, где текст для варианта доступен\n",
    "- распределение сентимента через **FinBERT**:\n",
    "  - средняя нейтральность `avg_neu_share` (по вероятности neutral)\n",
    "  - `non_neutral = 1 - avg_neu_share`\n",
    "  - интенсивность `mean_abs_score = mean(|p_pos - p_neg|)`\n",
    "\n",
    "### Как выбираем лучший вариант\n",
    "Композитный балл:\n",
    "\n",
    "`score = coverage * non_neutral * mean_abs_score`\n",
    "\n",
    "и выбираем максимум **отдельно для каждого источника**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ccb80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "FinBERT eval: source=all_external\n",
      "========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FinBERT variants: all_external: 66it [00:44,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top variants for all_external\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>variant</th>\n",
       "      <th>rows_after_filters</th>\n",
       "      <th>rows_with_text</th>\n",
       "      <th>coverage</th>\n",
       "      <th>avg_len_chars</th>\n",
       "      <th>avg_neu_share</th>\n",
       "      <th>non_neutral</th>\n",
       "      <th>mean_abs_score</th>\n",
       "      <th>composite_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_external</td>\n",
       "      <td>title_article</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_external</td>\n",
       "      <td>lsa_summary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_external</td>\n",
       "      <td>luhn_summary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all_external</td>\n",
       "      <td>textrank_summary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all_external</td>\n",
       "      <td>lexrank_summary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         source           variant  rows_after_filters  rows_with_text  coverage  avg_len_chars  avg_neu_share  non_neutral  mean_abs_score  composite_score\n",
       "0  all_external     title_article                   0               0       0.0            0.0            1.0          0.0             0.0              0.0\n",
       "1  all_external       lsa_summary                   0               0       0.0            0.0            1.0          0.0             0.0              0.0\n",
       "2  all_external      luhn_summary                   0               0       0.0            0.0            1.0          0.0             0.0              0.0\n",
       "3  all_external  textrank_summary                   0               0       0.0            0.0            1.0          0.0             0.0              0.0\n",
       "4  all_external   lexrank_summary                   0               0       0.0            0.0            1.0          0.0             0.0              0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "FinBERT eval: source=nasdaq\n",
      "========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FinBERT variants: nasdaq: 78it [55:22:37, 2555.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top variants for nasdaq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>variant</th>\n",
       "      <th>rows_after_filters</th>\n",
       "      <th>rows_with_text</th>\n",
       "      <th>coverage</th>\n",
       "      <th>avg_len_chars</th>\n",
       "      <th>avg_neu_share</th>\n",
       "      <th>non_neutral</th>\n",
       "      <th>mean_abs_score</th>\n",
       "      <th>composite_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nasdaq</td>\n",
       "      <td>title_article</td>\n",
       "      <td>1251664</td>\n",
       "      <td>918413</td>\n",
       "      <td>0.733754</td>\n",
       "      <td>5252.162846</td>\n",
       "      <td>0.380678</td>\n",
       "      <td>0.619322</td>\n",
       "      <td>0.519120</td>\n",
       "      <td>0.235904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nasdaq</td>\n",
       "      <td>lsa_summary</td>\n",
       "      <td>1251664</td>\n",
       "      <td>918412</td>\n",
       "      <td>0.733753</td>\n",
       "      <td>574.253755</td>\n",
       "      <td>0.386465</td>\n",
       "      <td>0.613535</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.234087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nasdaq</td>\n",
       "      <td>lexrank_summary</td>\n",
       "      <td>1251664</td>\n",
       "      <td>918413</td>\n",
       "      <td>0.733754</td>\n",
       "      <td>520.792258</td>\n",
       "      <td>0.408596</td>\n",
       "      <td>0.591404</td>\n",
       "      <td>0.501867</td>\n",
       "      <td>0.217783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nasdaq</td>\n",
       "      <td>luhn_summary</td>\n",
       "      <td>1251664</td>\n",
       "      <td>918413</td>\n",
       "      <td>0.733754</td>\n",
       "      <td>585.485694</td>\n",
       "      <td>0.435732</td>\n",
       "      <td>0.564268</td>\n",
       "      <td>0.476860</td>\n",
       "      <td>0.197436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nasdaq</td>\n",
       "      <td>textrank_summary</td>\n",
       "      <td>1251664</td>\n",
       "      <td>918413</td>\n",
       "      <td>0.733754</td>\n",
       "      <td>615.212608</td>\n",
       "      <td>0.444163</td>\n",
       "      <td>0.555837</td>\n",
       "      <td>0.470008</td>\n",
       "      <td>0.191692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source           variant  rows_after_filters  rows_with_text  coverage  avg_len_chars  avg_neu_share  non_neutral  mean_abs_score  composite_score\n",
       "0  nasdaq     title_article             1251664          918413  0.733754    5252.162846       0.380678     0.619322        0.519120         0.235904\n",
       "1  nasdaq       lsa_summary             1251664          918412  0.733753     574.253755       0.386465     0.613535        0.519983         0.234087\n",
       "2  nasdaq   lexrank_summary             1251664          918413  0.733754     520.792258       0.408596     0.591404        0.501867         0.217783\n",
       "3  nasdaq      luhn_summary             1251664          918413  0.733754     585.485694       0.435732     0.564268        0.476860         0.197436\n",
       "4  nasdaq  textrank_summary             1251664          918413  0.733754     615.212608       0.444163     0.555837        0.470008         0.191692"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs_final/text_variant_finbert_report.csv\n"
     ]
    }
   ],
   "source": [
    "TEXT_VARIANTS = [\"title_article\", \"lsa_summary\", \"luhn_summary\", \"textrank_summary\", \"lexrank_summary\"]\n",
    "\n",
    "def _load_hf_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(model_id).to(DEVICE)\n",
    "    mdl.eval()\n",
    "    raw = getattr(mdl.config, \"id2label\", {})\n",
    "    id2lab = {int(k): str(v).lower() for k, v in raw.items()} if isinstance(raw, dict) else {}\n",
    "    return tok, mdl, id2lab\n",
    "\n",
    "def _resolve_sentiment_slots(id2lab: dict):\n",
    "    def pick(keys):\n",
    "        for i, lab in id2lab.items():\n",
    "            lab2 = str(lab).lower()\n",
    "            for k in keys:\n",
    "                if k in lab2:\n",
    "                    return int(i)\n",
    "        return None\n",
    "\n",
    "    neg = pick([\"negative\", \"neg\", \"bearish\"])\n",
    "    pos = pick([\"positive\", \"pos\", \"bullish\"])\n",
    "    neu = pick([\"neutral\", \"neu\", \"none\"])\n",
    "    return neg, neu, pos\n",
    "\n",
    "def _hf_predict(texts: List[str], tok, mdl, neg_i, neu_i, pos_i,\n",
    "                batch_size: int = 64, max_length: int = 192):\n",
    "    n = len(texts)\n",
    "    pneg = np.zeros(n, dtype=np.float32)\n",
    "    pneu = np.zeros(n, dtype=np.float32)\n",
    "    ppos = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for a in range(0, n, batch_size):\n",
    "            b = texts[a:a + batch_size]\n",
    "            enc = tok(\n",
    "                b,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=int(max_length),\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "            logits = mdl(**enc).logits\n",
    "            pr = torch.softmax(logits, dim=-1).detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "            if neg_i is None or pos_i is None:\n",
    "                if pr.shape[1] == 2:\n",
    "                    pneg[a:a + len(b)] = pr[:, 0]\n",
    "                    ppos[a:a + len(b)] = pr[:, 1]\n",
    "                    pneu[a:a + len(b)] = 0.0\n",
    "                else:\n",
    "                    raise ValueError(\"Не удалось определить метки сентимента (neg/pos) для этой модели\")\n",
    "            else:\n",
    "                pneg[a:a + len(b)] = pr[:, int(neg_i)]\n",
    "                ppos[a:a + len(b)] = pr[:, int(pos_i)]\n",
    "                pneu[a:a + len(b)] = pr[:, int(neu_i)] if neu_i is not None else 0.0\n",
    "\n",
    "    score = (ppos - pneg).astype(np.float32)\n",
    "    abs_score = np.abs(score).astype(np.float32)\n",
    "    return score, abs_score, pneg, pneu, ppos\n",
    "\n",
    "def _is_dup_by_keys(keys_sorted: np.ndarray, keys: np.ndarray) -> np.ndarray:\n",
    "    idx = np.searchsorted(keys_sorted, keys)\n",
    "    mask = (idx < len(keys_sorted)) & (keys_sorted[idx] == keys)\n",
    "    return mask\n",
    "\n",
    "def eval_text_variants_finbert(source_tag: str, file_path: str,\n",
    "                              preferred_keys_sorted: Optional[np.ndarray],\n",
    "                              is_preferred_source: bool,\n",
    "                              batch_size: int = 64, max_length: int = 192,\n",
    "                              model_id: str = \"ProsusAI/finbert\") -> pd.DataFrame:\n",
    "    print_step(f\"FinBERT eval: source={source_tag}\")\n",
    "\n",
    "    tok, mdl, id2lab = _load_hf_model(model_id)\n",
    "    neg_i, neu_i, pos_i = _resolve_sentiment_slots(id2lab)\n",
    "\n",
    "    metrics = {v: {\"rows_after_filters\": 0, \"rows_with_text\": 0,\n",
    "                   \"len_sum\": 0, \"neu_sum\": 0.0, \"abs_score_sum\": 0.0}\n",
    "               for v in TEXT_VARIANTS}\n",
    "\n",
    "    reader = pd.read_csv(file_path, chunksize=CHUNK_SIZE, low_memory=False)\n",
    "\n",
    "    for raw in tqdm(reader, desc=f\"FinBERT variants: {source_tag}\"):\n",
    "        ch = _prep_news_chunk_base(raw, source_tag)\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        if (not is_preferred_source) and (preferred_keys_sorted is not None) and len(preferred_keys_sorted) > 0:\n",
    "            kh = _hash_url_ticker(ch)\n",
    "            dup_mask = _is_dup_by_keys(preferred_keys_sorted, kh)\n",
    "            if dup_mask.any():\n",
    "                ch = ch.loc[~dup_mask].reset_index(drop=True)\n",
    "                if ch.empty:\n",
    "                    continue\n",
    "\n",
    "        for v in TEXT_VARIANTS:\n",
    "            metrics[v][\"rows_after_filters\"] += len(ch)\n",
    "\n",
    "        for v in TEXT_VARIANTS:\n",
    "            text = build_text_variant(ch, v)\n",
    "            m = (text != \"\")\n",
    "            if not m.any():\n",
    "                continue\n",
    "\n",
    "            texts = text.loc[m].tolist()\n",
    "            metrics[v][\"rows_with_text\"] += len(texts)\n",
    "            metrics[v][\"len_sum\"] += int(pd.Series(texts).str.len().sum())\n",
    "\n",
    "            score, abs_score, pneg, pneu, ppos = _hf_predict(\n",
    "                texts, tok, mdl, neg_i, neu_i, pos_i,\n",
    "                batch_size=batch_size, max_length=max_length\n",
    "            )\n",
    "\n",
    "            metrics[v][\"neu_sum\"] += float(np.sum(pneu))\n",
    "            metrics[v][\"abs_score_sum\"] += float(np.sum(abs_score))\n",
    "\n",
    "        del ch\n",
    "        gc.collect()\n",
    "\n",
    "    rows = []\n",
    "    for v in TEXT_VARIANTS:\n",
    "        a = metrics[v][\"rows_after_filters\"]\n",
    "        n = metrics[v][\"rows_with_text\"]\n",
    "\n",
    "        coverage = (n / a) if a else 0.0\n",
    "        avg_len = (metrics[v][\"len_sum\"] / n) if n else 0.0\n",
    "        avg_neu = (metrics[v][\"neu_sum\"] / n) if n else 1.0\n",
    "        non_neutral = 1.0 - avg_neu\n",
    "        mean_abs_score = (metrics[v][\"abs_score_sum\"] / n) if n else 0.0\n",
    "\n",
    "        score = coverage * non_neutral * mean_abs_score\n",
    "\n",
    "        rows.append({\n",
    "            \"source\": source_tag,\n",
    "            \"variant\": v,\n",
    "            \"rows_after_filters\": int(a),\n",
    "            \"rows_with_text\": int(n),\n",
    "            \"coverage\": float(coverage),\n",
    "            \"avg_len_chars\": float(avg_len),\n",
    "            \"avg_neu_share\": float(avg_neu),\n",
    "            \"non_neutral\": float(non_neutral),\n",
    "            \"mean_abs_score\": float(mean_abs_score),\n",
    "            \"composite_score\": float(score),\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values(\"composite_score\", ascending=False).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "preferred_keys_sorted = None\n",
    "\n",
    "variant_reports = []\n",
    "for tag, fp in NEWS_SOURCES.items():\n",
    "    rep = eval_text_variants_finbert(\n",
    "        source_tag=tag,\n",
    "        file_path=fp,\n",
    "        preferred_keys_sorted=preferred_keys_sorted,\n",
    "        is_preferred_source=True,\n",
    "        batch_size=64,\n",
    "        max_length=192,\n",
    "        model_id=\"ProsusAI/finbert\"\n",
    "    )\n",
    "    variant_reports.append(rep)\n",
    "    print(\"Top variants for\", tag)\n",
    "    display(rep.head(10))\n",
    "\n",
    "variant_report = pd.concat(variant_reports, ignore_index=True)\n",
    "report_path = os.path.join(OUTPUT_DIR, \"text_variant_finbert_report.csv\")\n",
    "variant_report.to_csv(report_path, index=False)\n",
    "print(\"Saved:\", report_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de284aab",
   "metadata": {},
   "source": [
    "## Блок 8: Выбор лучшего варианта текста для каждого источника\n",
    "\n",
    "На основании отчета `text_variant_finbert_report.csv` выбираем по одному лучшему варианту на источник.\n",
    "\n",
    "Также сохраняем выбор в JSON, чтобы он был воспроизводим:\n",
    "`outputs_final/selected_text_variants.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf892b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected text variants per source:\n",
      "{\n",
      "  \"all_external\": \"title_article\",\n",
      "  \"nasdaq\": \"title_article\"\n",
      "}\n",
      "Saved: outputs_final/selected_text_variants.json\n"
     ]
    }
   ],
   "source": [
    "variant_report = pd.read_csv(os.path.join(OUTPUT_DIR, \"text_variant_finbert_report.csv\"))\n",
    "\n",
    "selected_variants = {}\n",
    "for src in variant_report[\"source\"].unique():\n",
    "    top = variant_report.loc[variant_report[\"source\"] == src].sort_values(\"composite_score\", ascending=False).iloc[0]\n",
    "    selected_variants[src] = str(top[\"variant\"])\n",
    "\n",
    "print(\"Selected text variants per source:\")\n",
    "print(json.dumps(selected_variants, indent=2))\n",
    "\n",
    "sel_path = os.path.join(OUTPUT_DIR, \"selected_text_variants.json\")\n",
    "with open(sel_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"preferred_source_for_duplicates\": preferred_source,\n",
    "        \"selected_variants\": selected_variants,\n",
    "        \"text_variants_considered\": TEXT_VARIANTS,\n",
    "        \"date_range\": [str(START_DATE.date()), str(END_DATE.date())],\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", sel_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb6d4d",
   "metadata": {},
   "source": [
    "## Блок 9: Доходности по акциям (`prices_returns_2019_2023.parquet`)\n",
    "\n",
    "Мы строим дневные лог-доходности:\n",
    "\n",
    "`r_t = ln(P_t) - ln(P_{t-1})`\n",
    "\n",
    "где P_t — цена = **Adj Close**, чтобы учитывать сплиты и дивиденды корректно\n",
    "\n",
    "Сохраняем результат в `outputs_final/prices_returns_2019_2023.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ab6a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "Сборка доходностей из CSV цен\n",
      "========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Чтение цен по тикерам: 100%|█████████████| 2193/2193 [00:12<00:00, 175.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранено: outputs_final/prices_returns_2019_2023.parquet\n",
      "Цены+доходности: строк 2,755,903, тикеров 2193\n",
      "Период: 2019-01-02 — 2023-12-28\n",
      "price_col_used (топ):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "price_col_used\n",
       "adj close    2755903\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ret_log NaN%: 0.07957464395517548\n",
      "0.001   -0.251266\n",
      "0.010   -0.097432\n",
      "0.500    0.000000\n",
      "0.990    0.098719\n",
      "0.999    0.240152\n",
      "Name: ret_log, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "BASE_RET_COLS = [\"ticker\", \"date\", \"price\", \"ret_log\", \"volume\", \"price_col_used\"]\n",
    "\n",
    "def _colmap_lower(df: pd.DataFrame) -> dict:\n",
    "    return {str(c).strip().lower(): c for c in df.columns}\n",
    "\n",
    "def _pick_price_col(df: pd.DataFrame) -> str:\n",
    "    cm = _colmap_lower(df)\n",
    "    candidates = [\"adj close\", \"adjusted close\", \"adj_close\", \"adjclose\", \"close\"]\n",
    "    for key in candidates:\n",
    "        if key in cm:\n",
    "            return cm[key]\n",
    "    raise ValueError(\"Price column not found\")\n",
    "\n",
    "def _pick_volume_col(df: pd.DataFrame) -> Optional[str]:\n",
    "    cm = _colmap_lower(df)\n",
    "    for key in [\"volume\", \"vol\"]:\n",
    "        if key in cm:\n",
    "            return cm[key]\n",
    "    return None\n",
    "\n",
    "def build_prices_returns(prices_dir: str, out_path: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    missing_files = 0\n",
    "    skipped_bad = 0\n",
    "\n",
    "    for t in tqdm(sorted(TICKER_SET), desc=\"Reading prices\"):\n",
    "        fp = os.path.join(prices_dir, f\"{t}.csv\")\n",
    "        if not os.path.exists(fp):\n",
    "            missing_files += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                fp,\n",
    "                usecols=lambda c: str(c).strip().lower() in {\"date\", \"adj close\", \"close\", \"volume\"},\n",
    "                low_memory=False\n",
    "            )\n",
    "        except Exception:\n",
    "            skipped_bad += 1\n",
    "            continue\n",
    "\n",
    "        if \"Date\" in df.columns and \"date\" not in df.columns:\n",
    "            df = df.rename(columns={\"Date\": \"date\"})\n",
    "        elif \"date\" not in df.columns:\n",
    "            cm = _colmap_lower(df)\n",
    "            if \"date\" in cm:\n",
    "                df = df.rename(columns={cm[\"date\"]: \"date\"})\n",
    "            else:\n",
    "                skipped_bad += 1\n",
    "                continue\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "        df = df.dropna(subset=[\"date\"])\n",
    "        df = df[(df[\"date\"] >= START_DATE) & (df[\"date\"] <= END_DATE)]\n",
    "        df = df.drop_duplicates(subset=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            price_col = _pick_price_col(df)\n",
    "        except Exception:\n",
    "            skipped_bad += 1\n",
    "            continue\n",
    "\n",
    "        df[\"price\"] = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"price\"])\n",
    "        df = df[df[\"price\"] > 0]\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df[\"ret_log\"] = np.log(df[\"price\"]).diff()\n",
    "\n",
    "        vol_col = _pick_volume_col(df)\n",
    "        if vol_col is not None:\n",
    "            df[\"volume\"] = pd.to_numeric(df[vol_col], errors=\"coerce\")\n",
    "        else:\n",
    "            df[\"volume\"] = np.nan\n",
    "\n",
    "        df[\"ticker\"] = str(t)\n",
    "        df[\"price_col_used\"] = str(price_col)\n",
    "\n",
    "        rows.append(df[BASE_RET_COLS])\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"No valid price files found\")\n",
    "\n",
    "    ret = (\n",
    "        pd.concat(rows, ignore_index=True)\n",
    "          .sort_values([\"ticker\", \"date\"], kind=\"mergesort\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    ret.to_parquet(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    if missing_files:\n",
    "        print(\"Missing ticker files:\", int(missing_files))\n",
    "    if skipped_bad:\n",
    "        print(\"Skipped due to format/read errors:\", int(skipped_bad))\n",
    "\n",
    "    return ret\n",
    "\n",
    "RET_PATH = os.path.join(OUTPUT_DIR, \"prices_returns_2019_2023.parquet\")\n",
    "\n",
    "print_step(\"Building price returns\")\n",
    "if os.path.exists(RET_PATH):\n",
    "    ret = pd.read_parquet(RET_PATH)\n",
    "else:\n",
    "    ret = build_prices_returns(PRICES_DIR, RET_PATH)\n",
    "\n",
    "ret = ret[BASE_RET_COLS].copy()\n",
    "ret[\"ticker\"] = ret[\"ticker\"].astype(str).str.strip()\n",
    "ret[\"date\"] = pd.to_datetime(ret[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "ret = ret.dropna(subset=[\"ticker\", \"date\"])\n",
    "ret = ret[ret[\"ticker\"].isin(TICKER_SET)].copy()\n",
    "ret = ret.sort_values([\"ticker\", \"date\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "print(\"Rows:\", f\"{len(ret):,}\", \"Tickers:\", int(ret[\"ticker\"].nunique()))\n",
    "print(\"Date range:\", ret[\"date\"].min().date(), \"—\", ret[\"date\"].max().date())\n",
    "display(ret[\"price_col_used\"].value_counts().head(10))\n",
    "print(\"ret_log NaN%:\", float(ret[\"ret_log\"].isna().mean()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3f71a",
   "metadata": {},
   "source": [
    "## Блок 10: Рыночная доходность и избыточные доходности\n",
    "\n",
    "Чтобы отделить общий рыночный шум, используем рыночный прокси `R_m,t` \n",
    "\n",
    "Будем использовать **SPY**\n",
    "\n",
    "Избыточная лог‑доходность:\n",
    "\n",
    "`excess_ret_log = ret_log - mkt_ret_log`\n",
    "\n",
    "Если SPY недоступен, используем равновзвешенную доходность по всем тикерам выборки на дату"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1881020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "Рыночная доходность + excess_ret_log\n",
      "========================================================================================\n",
      "Market proxy: SPY (через yfinance)\n",
      "Доля строк, где mkt_ret_log не NaN: 99.9%\n",
      "Доля строк, где excess_ret_log не NaN: 99.9%\n"
     ]
    }
   ],
   "source": [
    "MARKET_TICKER = \"SPY\"\n",
    "USE_YFINANCE_MARKET = False\n",
    "\n",
    "def _make_unique_cols(cols):\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        c = str(c).strip()\n",
    "        if c not in seen:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}__{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        flat = []\n",
    "        for tup in df.columns.to_list():\n",
    "            parts = [str(x) for x in tup if x not in [None, \"\", \" \"]]\n",
    "            flat.append(\"_\".join(parts))\n",
    "        df.columns = _make_unique_cols(flat)\n",
    "    else:\n",
    "        df.columns = _make_unique_cols([str(c).strip() for c in df.columns])\n",
    "    return df\n",
    "\n",
    "def _find_col_by_keywords(columns, keywords):\n",
    "    for c in columns:\n",
    "        s = str(c).lower()\n",
    "        if all(k in s for k in keywords):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _market_returns_from_prices_dir(prices_dir: str, ticker: str) -> pd.DataFrame:\n",
    "    fp = os.path.join(prices_dir, f\"{ticker}.csv\")\n",
    "    if not os.path.exists(fp):\n",
    "        raise FileNotFoundError(fp)\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        fp,\n",
    "        usecols=lambda c: str(c).strip().lower() in {\"date\", \"adj close\", \"close\"},\n",
    "        low_memory=False\n",
    "    )\n",
    "    if \"Date\" in df.columns and \"date\" not in df.columns:\n",
    "        df = df.rename(columns={\"Date\": \"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "    df = df.dropna(subset=[\"date\"]).drop_duplicates(subset=[\"date\"]).sort_values(\"date\")\n",
    "    df = df[(df[\"date\"] >= START_DATE) & (df[\"date\"] <= END_DATE)]\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Empty market series\")\n",
    "\n",
    "    price_col = _pick_price_col(df)\n",
    "    df[\"mkt_price\"] = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"mkt_price\"]).copy()\n",
    "    df[\"mkt_ret_log\"] = np.log(df[\"mkt_price\"]).diff()\n",
    "    return df[[\"date\", \"mkt_price\", \"mkt_ret_log\"]]\n",
    "\n",
    "def _market_returns_from_yfinance(ticker: str) -> pd.DataFrame:\n",
    "    import yfinance as yf\n",
    "\n",
    "    df = yf.download(\n",
    "        ticker,\n",
    "        start=str(START_DATE.date()),\n",
    "        end=str((END_DATE + pd.Timedelta(days=1)).date()),\n",
    "        progress=False,\n",
    "        auto_adjust=False,\n",
    "        group_by=\"column\",\n",
    "    )\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"Empty yfinance data\")\n",
    "\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [str(t[0]) for t in df.columns.to_list()]\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df = _flatten_columns(df)\n",
    "\n",
    "    date_col = \"Date\" if \"Date\" in df.columns else (\"Datetime\" if \"Datetime\" in df.columns else df.columns[0])\n",
    "\n",
    "    price_col = _find_col_by_keywords(df.columns, [\"adj\", \"close\"])\n",
    "    if price_col is None:\n",
    "        price_col = _find_col_by_keywords(df.columns, [\"close\"])\n",
    "    if price_col is None:\n",
    "        raise KeyError(\"Close column not found\")\n",
    "\n",
    "    df = df.rename(columns={date_col: \"date\", price_col: \"mkt_price\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "    df[\"mkt_price\"] = pd.to_numeric(df[\"mkt_price\"], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[\"date\", \"mkt_price\"]).sort_values(\"date\").drop_duplicates(subset=[\"date\"])\n",
    "    df = df[(df[\"date\"] >= START_DATE) & (df[\"date\"] <= END_DATE)]\n",
    "    df[\"mkt_ret_log\"] = np.log(df[\"mkt_price\"]).diff()\n",
    "    return df[[\"date\", \"mkt_price\", \"mkt_ret_log\"]]\n",
    "\n",
    "print_step(\"Market return + excess_ret_log\")\n",
    "\n",
    "try:\n",
    "    mkt = _market_returns_from_prices_dir(PRICES_DIR, MARKET_TICKER)\n",
    "    print(\"Market proxy:\", MARKET_TICKER, \"(from prices directory)\")\n",
    "except Exception as e:\n",
    "    if USE_YFINANCE_MARKET:\n",
    "        mkt = _market_returns_from_yfinance(MARKET_TICKER)\n",
    "        print(\"Market proxy:\", MARKET_TICKER, \"(yfinance)\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "mkt = _flatten_columns(mkt)\n",
    "mkt[\"date\"] = pd.to_datetime(mkt[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "mkt = mkt.dropna(subset=[\"date\"]).drop_duplicates(subset=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "ret = _flatten_columns(ret)\n",
    "_drop_cols = [c for c in ret.columns if str(c).startswith(\"mkt_\") or str(c).startswith(\"excess_ret_log\")]\n",
    "ret = ret.drop(columns=_drop_cols, errors=\"ignore\")\n",
    "\n",
    "_mkt_map = mkt.set_index(\"date\")[\"mkt_ret_log\"]\n",
    "ret[\"mkt_ret_log\"] = ret[\"date\"].map(_mkt_map)\n",
    "ret[\"excess_ret_log\"] = ret[\"ret_log\"] - ret[\"mkt_ret_log\"]\n",
    "\n",
    "print(\"mkt_ret_log non-NaN%:\", float(ret[\"mkt_ret_log\"].notna().mean()*100))\n",
    "print(\"excess_ret_log non-NaN%:\", float(ret[\"excess_ret_log\"].notna().mean()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf268bb",
   "metadata": {},
   "source": [
    "## Блок 11: Целевые переменные (горизонты 1/2/3/5 дней)\n",
    "\n",
    "Нам нужны будущие реакции рынка на новости\n",
    "\n",
    "Определим цели как сумму будущих **избыточных** доходностей:\n",
    "\n",
    "`y1_ex = excess_ret_log(t+1)`\n",
    "\n",
    "`y2_ex = excess_ret_log(t+1) + excess_ret_log(t+2)`\n",
    "\n",
    "`y3_ex = excess_ret_log(t+1) + excess_ret_log(t+2) + excess_ret_log(t+3)`\n",
    "\n",
    "`y5_ex = excess_ret_log(t+1) + excess_ret_log(t+2) + excess_ret_log(t+3) + excess_ret_log(t+4) + excess_ret_log(t+5)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d782a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "Цели y1/y2/y3/y5 по excess_ret_log\n",
      "========================================================================================\n",
      "y1_ex: доля NaN = 0.08%\n",
      "y2_ex: доля NaN = 0.16%\n",
      "y3_ex: доля NaN = 0.24%\n",
      "y5_ex: доля NaN = 0.40%\n"
     ]
    }
   ],
   "source": [
    "print_step(\"Цели y1/y2/y3/y5 по excess_ret_log\")\n",
    "\n",
    "ret = ret.sort_values([\"ticker\", \"date\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "g_ex = ret.groupby(\"ticker\", sort=False)[\"excess_ret_log\"]\n",
    "\n",
    "s1 = g_ex.shift(-1)\n",
    "s2 = g_ex.shift(-2)\n",
    "s3 = g_ex.shift(-3)\n",
    "s4 = g_ex.shift(-4)\n",
    "s5 = g_ex.shift(-5)\n",
    "\n",
    "ret[\"y1_ex\"] = s1.astype(\"float32\")\n",
    "ret[\"y2_ex\"] = (s1 + s2).astype(\"float32\")\n",
    "ret[\"y3_ex\"] = (s1 + s2 + s3).astype(\"float32\")\n",
    "ret[\"y5_ex\"] = (s1 + s2 + s3 + s4 + s5).astype(\"float32\")\n",
    "\n",
    "for c in [\"y1_ex\", \"y2_ex\", \"y3_ex\", \"y5_ex\"]:\n",
    "    print(f\"{c}: доля NaN = {ret[c].isna().mean()*100:.2f}%\")\n",
    "\n",
    "trade = ret[[\"ticker\", \"date\"]].rename(columns={\"date\": \"trade_date\"}).copy()\n",
    "trade = trade.sort_values([\"ticker\", \"trade_date\"], kind=\"mergesort\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e90037",
   "metadata": {},
   "source": [
    "## Блок 12: Построение дневных агрегатов сентимента (8 моделей) по выбранным текстам\n",
    "\n",
    "Когда для каждого источника выбран лучший вариант текста (`selected_text_variants.json`), строим финальные дневные агрегаты сентимента\n",
    "\n",
    "- для preferred источника считаем все новости как есть,\n",
    "- для второго источника пропускаем новости, чьи `(url,ticker)` уже встречались в preferred,\n",
    "- агрегирование по `(ticker, day)`, затем суммируем источники на уровне сырой агрегации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c3f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferred source: nasdaq\n",
      "Selected variants: {'all_external': 'title_article', 'nasdaq': 'title_article'}\n",
      "Preferred key count: 1251590\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, \"selected_text_variants.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    sel_meta = json.load(f)\n",
    "\n",
    "selected_variants = sel_meta[\"selected_variants\"]\n",
    "preferred_source = sel_meta.get(\"preferred_source_for_duplicates\") or list(selected_variants.keys())[0]\n",
    "preferred_keys_sorted = None\n",
    "\n",
    "print(\"Preferred source:\", preferred_source)\n",
    "print(\"Selected variants:\", selected_variants)\n",
    "\n",
    "def _checkpoint_path(prefix: str, source_tag: str, variant: str) -> str:\n",
    "    return os.path.join(CHECKPOINT_DIR, f\"{prefix}__{source_tag}__{variant}__rawagg.parquet\")\n",
    "\n",
    "def _reduce_parts(parts: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    if not parts:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat(parts, ignore_index=True)\n",
    "    df[\"ticker\"] = df[\"ticker\"].astype(str).str.strip()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.normalize()\n",
    "    df = df.dropna(subset=[\"ticker\", \"date\"])\n",
    "    df = df.groupby([\"ticker\", \"date\"], sort=False).sum(numeric_only=True).reset_index()\n",
    "    return df.sort_values([\"ticker\", \"date\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "def _prep_news_chunk_variant(raw: pd.DataFrame, source_tag: str, variant: str,\n",
    "                            skip_keys_sorted: Optional[np.ndarray]) -> pd.DataFrame:\n",
    "    ch = _prep_news_chunk_base(raw, source_tag)\n",
    "    if ch.empty:\n",
    "        return ch.iloc[0:0].copy()\n",
    "\n",
    "    if (skip_keys_sorted is not None) and len(skip_keys_sorted) > 0:\n",
    "        kh = _hash_url_ticker(ch)\n",
    "        dup_mask = _is_dup_by_keys(skip_keys_sorted, kh)\n",
    "        if dup_mask.any():\n",
    "            ch = ch.loc[~dup_mask].reset_index(drop=True)\n",
    "            if ch.empty:\n",
    "                return ch.iloc[0:0].copy()\n",
    "\n",
    "    text = build_text_variant(ch, variant)\n",
    "    m = (text != \"\")\n",
    "    if not m.any():\n",
    "        return ch.iloc[0:0].copy()\n",
    "\n",
    "    out = ch.loc[m, [\"ticker\", \"day\"]].copy()\n",
    "    out[\"text\"] = text.loc[m].astype(str)\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "def _vader_scorer():\n",
    "    try:\n",
    "        return SentimentIntensityAnalyzer()\n",
    "    except Exception:\n",
    "        nltk.download(\"vader_lexicon\", quiet=True)\n",
    "        return SentimentIntensityAnalyzer()\n",
    "\n",
    "def _vader_file_rawagg(file_path: str, source_tag: str, variant: str,\n",
    "                       skip_keys_sorted: Optional[np.ndarray],\n",
    "                       band: float, sia: SentimentIntensityAnalyzer,\n",
    "                       prefix: str = \"vader\") -> pd.DataFrame:\n",
    "    parts = []\n",
    "    buffer = []\n",
    "    reader = pd.read_csv(file_path, chunksize=CHUNK_SIZE, low_memory=False)\n",
    "\n",
    "    for raw in tqdm(reader, desc=f\"{prefix}: {source_tag}\"):\n",
    "        ch = _prep_news_chunk_variant(raw, source_tag, variant, skip_keys_sorted)\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        texts = ch[\"text\"].astype(str).tolist()\n",
    "        scores = np.fromiter((float(sia.polarity_scores(x)[\"compound\"]) for x in texts), dtype=np.float32, count=len(texts))\n",
    "        neg = (scores <= -float(band)).astype(np.int32)\n",
    "        pos = (scores >= float(band)).astype(np.int32)\n",
    "\n",
    "        tmp = ch[[\"ticker\", \"day\"]].copy()\n",
    "        tmp = tmp.rename(columns={\"day\": \"date\"})\n",
    "        tmp[\"n\"] = 1\n",
    "        tmp[\"ssum\"] = scores.astype(np.float32)\n",
    "        tmp[\"nneg\"] = neg\n",
    "        tmp[\"npos\"] = pos\n",
    "\n",
    "        g = tmp.groupby([\"ticker\", \"date\"], sort=False).sum(numeric_only=True).reset_index()\n",
    "        buffer.append(g)\n",
    "\n",
    "        if len(buffer) >= 25:\n",
    "            parts.append(_reduce_parts(buffer))\n",
    "            buffer = []\n",
    "\n",
    "    if buffer:\n",
    "        parts.append(_reduce_parts(buffer))\n",
    "\n",
    "    out = _reduce_parts(parts)\n",
    "    return out\n",
    "\n",
    "def build_vader_daily(band: float = 0.02) -> pd.DataFrame:\n",
    "    sia = _vader_scorer()\n",
    "\n",
    "    parts = []\n",
    "    for tag, fp in NEWS_SOURCES.items():\n",
    "        variant = selected_variants[tag]\n",
    "        skip = None if tag == preferred_source else preferred_keys_sorted\n",
    "\n",
    "        cp = _checkpoint_path(\"vader\", tag, variant)\n",
    "        if RESUME_FROM_CHECKPOINTS and os.path.exists(cp) and not FORCE_RECOMPUTE_CHECKPOINTS:\n",
    "            part = pd.read_parquet(cp)\n",
    "        else:\n",
    "            part = _vader_file_rawagg(fp, tag, variant, skip, band=band, sia=sia)\n",
    "            part.to_parquet(cp, index=False)\n",
    "        parts.append(part)\n",
    "\n",
    "    agg = _reduce_parts(parts)\n",
    "    if agg.empty:\n",
    "        raise ValueError(\"VADER: empty result\")\n",
    "\n",
    "    g = agg.copy()\n",
    "    g[\"vader_news_count\"] = g[\"n\"].round().astype(\"int32\")\n",
    "    g[\"vader_score_mean\"] = np.where(g[\"n\"] > 0, g[\"ssum\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[\"vader_neg_share\"] = np.where(g[\"n\"] > 0, g[\"nneg\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[\"vader_pos_share\"] = np.where(g[\"n\"] > 0, g[\"npos\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[\"vader_neu_share\"] = (1.0 - g[\"vader_neg_share\"] - g[\"vader_pos_share\"]).clip(0.0, 1.0).astype(\"float32\")\n",
    "    g[\"vader_active_score_mean\"] = g[\"vader_score_mean\"]\n",
    "    g = g.drop(columns=[\"n\", \"ssum\", \"nneg\", \"npos\"])\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, \"daily_news_vader_2019_2023.parquet\")\n",
    "    g.to_parquet(out_path, index=False)\n",
    "    print(\"Saved:\", out_path, \"Rows:\", f\"{len(g):,}\")\n",
    "    return g\n",
    "\n",
    "def _textblob_file_rawagg(file_path: str, source_tag: str, variant: str,\n",
    "                          skip_keys_sorted: Optional[np.ndarray],\n",
    "                          band: float, prefix: str = \"textblob\") -> pd.DataFrame:\n",
    "    parts = []\n",
    "    buffer = []\n",
    "    reader = pd.read_csv(file_path, chunksize=CHUNK_SIZE, low_memory=False)\n",
    "\n",
    "    def _tb(text: str) -> float:\n",
    "        try:\n",
    "            return float(TextBlob(text).sentiment.polarity)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    for raw in tqdm(reader, desc=f\"{prefix}: {source_tag}\"):\n",
    "        ch = _prep_news_chunk_variant(raw, source_tag, variant, skip_keys_sorted)\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        texts = ch[\"text\"].astype(str).tolist()\n",
    "        scores = np.fromiter((_tb(x) for x in texts), dtype=np.float32, count=len(texts))\n",
    "        neg = (scores <= -float(band)).astype(np.int32)\n",
    "        pos = (scores >= float(band)).astype(np.int32)\n",
    "\n",
    "        tmp = ch[[\"ticker\", \"day\"]].copy()\n",
    "        tmp = tmp.rename(columns={\"day\": \"date\"})\n",
    "        tmp[\"n\"] = 1\n",
    "        tmp[\"ssum\"] = scores.astype(np.float32)\n",
    "        tmp[\"nneg\"] = neg\n",
    "        tmp[\"npos\"] = pos\n",
    "\n",
    "        g = tmp.groupby([\"ticker\", \"date\"], sort=False).sum(numeric_only=True).reset_index()\n",
    "        buffer.append(g)\n",
    "\n",
    "        if len(buffer) >= 25:\n",
    "            parts.append(_reduce_parts(buffer))\n",
    "            buffer = []\n",
    "\n",
    "    if buffer:\n",
    "        parts.append(_reduce_parts(buffer))\n",
    "\n",
    "    out = _reduce_parts(parts)\n",
    "    return out\n",
    "\n",
    "def build_textblob_daily(band: float = 0.02) -> pd.DataFrame:\n",
    "    parts = []\n",
    "    for tag, fp in NEWS_SOURCES.items():\n",
    "        variant = selected_variants[tag]\n",
    "        skip = None if tag == preferred_source else preferred_keys_sorted\n",
    "\n",
    "        cp = _checkpoint_path(\"textblob\", tag, variant)\n",
    "        if RESUME_FROM_CHECKPOINTS and os.path.exists(cp) and not FORCE_RECOMPUTE_CHECKPOINTS:\n",
    "            part = pd.read_parquet(cp)\n",
    "        else:\n",
    "            part = _textblob_file_rawagg(fp, tag, variant, skip, band=band)\n",
    "            part.to_parquet(cp, index=False)\n",
    "        parts.append(part)\n",
    "\n",
    "    agg = _reduce_parts(parts)\n",
    "    if agg.empty:\n",
    "        raise ValueError(\"TextBlob: empty result\")\n",
    "\n",
    "    g = agg.copy()\n",
    "    g[\"textblob_news_count\"] = g[\"n\"].round().astype(\"int32\")\n",
    "    g[\"textblob_score_mean\"] = np.where(g[\"n\"] > 0, g[\"ssum\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[\"textblob_neg_share\"] = np.where(g[\"n\"] > 0, g[\"nneg\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[\"textblob_pos_share\"] = np.where(g[\"n\"] > 0, g[\"npos\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[\"textblob_neu_share\"] = (1.0 - g[\"textblob_neg_share\"] - g[\"textblob_pos_share\"]).clip(0.0, 1.0).astype(\"float32\")\n",
    "    g[\"textblob_active_score_mean\"] = g[\"textblob_score_mean\"]\n",
    "    g = g.drop(columns=[\"n\", \"ssum\", \"nneg\", \"npos\"])\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, \"daily_news_textblob_2019_2023.parquet\")\n",
    "    g.to_parquet(out_path, index=False)\n",
    "    print(\"Saved:\", out_path, \"Rows:\", f\"{len(g):,}\")\n",
    "    return g\n",
    "\n",
    "def _hf_file_rawagg(file_path: str, source_tag: str, variant: str,\n",
    "                    skip_keys_sorted: Optional[np.ndarray],\n",
    "                    tok, mdl, neg_i, neu_i, pos_i,\n",
    "                    batch_size: int = 64, max_length: int = 192,\n",
    "                    prefix: str = \"hf\") -> pd.DataFrame:\n",
    "    parts = []\n",
    "    buffer = []\n",
    "    reader = pd.read_csv(file_path, chunksize=CHUNK_SIZE, low_memory=False)\n",
    "\n",
    "    for raw in tqdm(reader, desc=f\"{prefix}: {source_tag}\"):\n",
    "        ch = _prep_news_chunk_variant(raw, source_tag, variant, skip_keys_sorted)\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        texts = ch[\"text\"].astype(str).tolist()\n",
    "        score, abs_score, pneg, pneu, ppos = _hf_predict(\n",
    "            texts, tok, mdl, neg_i, neu_i, pos_i,\n",
    "            batch_size=batch_size, max_length=max_length\n",
    "        )\n",
    "\n",
    "        mass = (ppos + pneg).astype(np.float32)\n",
    "        active = (score / (mass + 1e-9)).astype(np.float32)\n",
    "        pneg_a = (pneg / (mass + 1e-9)).astype(np.float32)\n",
    "        ppos_a = (ppos / (mass + 1e-9)).astype(np.float32)\n",
    "\n",
    "        tmp = ch[[\"ticker\", \"day\"]].copy()\n",
    "        tmp = tmp.rename(columns={\"day\": \"date\"})\n",
    "        tmp[\"n\"] = 1\n",
    "        tmp[\"ssum\"] = score.astype(np.float32)\n",
    "        tmp[\"asum\"] = active.astype(np.float32)\n",
    "        tmp[\"neg\"] = pneg.astype(np.float32)\n",
    "        tmp[\"neu\"] = pneu.astype(np.float32)\n",
    "        tmp[\"pos\"] = ppos.astype(np.float32)\n",
    "        tmp[\"neg_a\"] = pneg_a.astype(np.float32)\n",
    "        tmp[\"pos_a\"] = ppos_a.astype(np.float32)\n",
    "\n",
    "        g = tmp.groupby([\"ticker\", \"date\"], sort=False).sum(numeric_only=True).reset_index()\n",
    "        buffer.append(g)\n",
    "\n",
    "        if len(buffer) >= 25:\n",
    "            parts.append(_reduce_parts(buffer))\n",
    "            buffer = []\n",
    "\n",
    "    if buffer:\n",
    "        parts.append(_reduce_parts(buffer))\n",
    "\n",
    "    out = _reduce_parts(parts)\n",
    "    return out\n",
    "\n",
    "def build_hf_daily(model_id: str, prefix: str,\n",
    "                   batch_size: int = 64, max_length: int = 192) -> pd.DataFrame:\n",
    "    tok, mdl, id2lab = _load_hf_model(model_id)\n",
    "    neg_i, neu_i, pos_i = _resolve_sentiment_slots(id2lab)\n",
    "\n",
    "    parts = []\n",
    "    for tag, fp in NEWS_SOURCES.items():\n",
    "        variant = selected_variants[tag]\n",
    "        skip = None if tag == preferred_source else preferred_keys_sorted\n",
    "\n",
    "        cp = _checkpoint_path(prefix, tag, variant)\n",
    "        if RESUME_FROM_CHECKPOINTS and os.path.exists(cp) and not FORCE_RECOMPUTE_CHECKPOINTS:\n",
    "            part = pd.read_parquet(cp)\n",
    "        else:\n",
    "            part = _hf_file_rawagg(\n",
    "                fp, tag, variant, skip,\n",
    "                tok, mdl, neg_i, neu_i, pos_i,\n",
    "                batch_size=batch_size, max_length=max_length,\n",
    "                prefix=prefix\n",
    "            )\n",
    "            part.to_parquet(cp, index=False)\n",
    "        parts.append(part)\n",
    "\n",
    "    agg = _reduce_parts(parts)\n",
    "    if agg.empty:\n",
    "        raise ValueError(f\"{prefix}: empty result\")\n",
    "\n",
    "    g = agg.copy()\n",
    "    g[f\"{prefix}_news_count\"] = g[\"n\"].round().astype(\"int32\")\n",
    "    g[f\"{prefix}_score_mean\"] = np.where(g[\"n\"] > 0, g[\"ssum\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[f\"{prefix}_neg_share\"] = np.where(g[\"n\"] > 0, g[\"neg\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[f\"{prefix}_neu_share\"] = np.where(g[\"n\"] > 0, g[\"neu\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[f\"{prefix}_pos_share\"] = np.where(g[\"n\"] > 0, g[\"pos\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "\n",
    "    g[f\"{prefix}_active_score_mean\"] = np.where(g[\"n\"] > 0, g[\"asum\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[f\"{prefix}_active_neg_share\"] = np.where(g[\"n\"] > 0, g[\"neg_a\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "    g[f\"{prefix}_active_pos_share\"] = np.where(g[\"n\"] > 0, g[\"pos_a\"] / g[\"n\"], np.nan).astype(\"float32\")\n",
    "\n",
    "    g = g.drop(columns=[\"n\", \"ssum\", \"asum\", \"neg\", \"neu\", \"pos\", \"neg_a\", \"pos_a\"])\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"daily_news_{prefix}_2019_2023.parquet\")\n",
    "    g.to_parquet(out_path, index=False)\n",
    "    print(\"Saved:\", out_path, \"Rows:\", f\"{len(g):,}\")\n",
    "\n",
    "    del mdl\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4792337c",
   "metadata": {},
   "source": [
    "## Блок 13: Запуск 8 моделей и сохранение `daily_news_*.parquet`\n",
    "\n",
    "- VADER\n",
    "- TextBlob\n",
    "- 6 transformer‑моделей (FinBERT + альтернативы)\n",
    "\n",
    "Результат: по каждой модели файл в `outputs_final/`:\n",
    "`daily_news_<model>_2019_2023.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de8e993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "Сборка дневных агрегатов сентимента (8 моделей)\n",
      "========================================================================================\n",
      "[vader] найден итоговый файл -> outputs_final/daily_news_vader_2019_2023.parquet | строк: 432,985\n",
      "[textblob] найден итоговый файл -> outputs_final/daily_news_textblob_2019_2023.parquet | строк: 432,985\n",
      "[finbert] найден итоговый файл -> outputs_final/daily_news_finbert_2019_2023.parquet | строк: 432,985\n",
      "[finbert_tone] найден итоговый файл -> outputs_final/daily_news_finbert_tone_2019_2023.parquet | строк: 432,985\n",
      "[finroberta] найден итоговый файл -> outputs_final/daily_news_finroberta_2019_2023.parquet | строк: 432,985\n",
      "[distilroberta_finnews] найден итоговый файл -> outputs_final/daily_news_distilroberta_finnews_2019_2023.parquet | строк: 432,985\n",
      "[deberta_finnews] найден итоговый файл -> outputs_final/daily_news_deberta_finnews_2019_2023.parquet | строк: 432,985\n",
      "[twroberta] найден итоговый файл -> outputs_final/daily_news_twroberta_2019_2023.parquet | строк: 432,985\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>finbert_news_count</th>\n",
       "      <th>finbert_score_mean</th>\n",
       "      <th>finbert_active_score_mean</th>\n",
       "      <th>finbert_neg_share</th>\n",
       "      <th>finbert_neu_share</th>\n",
       "      <th>finbert_pos_share</th>\n",
       "      <th>finbert_active_neg_share</th>\n",
       "      <th>finbert_active_pos_share</th>\n",
       "      <th>news_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-09-11</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.059356</td>\n",
       "      <td>-0.396502</td>\n",
       "      <td>0.104527</td>\n",
       "      <td>0.850301</td>\n",
       "      <td>0.045172</td>\n",
       "      <td>0.698251</td>\n",
       "      <td>0.301749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-09-15</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.927253</td>\n",
       "      <td>-0.947452</td>\n",
       "      <td>0.952966</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.025714</td>\n",
       "      <td>0.973726</td>\n",
       "      <td>0.026274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-09-16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.056851</td>\n",
       "      <td>0.610298</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.910010</td>\n",
       "      <td>0.073420</td>\n",
       "      <td>0.194851</td>\n",
       "      <td>0.805149</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>0.283029</td>\n",
       "      <td>0.023983</td>\n",
       "      <td>0.933100</td>\n",
       "      <td>0.042917</td>\n",
       "      <td>0.358485</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.585094</td>\n",
       "      <td>0.967716</td>\n",
       "      <td>0.009760</td>\n",
       "      <td>0.395387</td>\n",
       "      <td>0.594854</td>\n",
       "      <td>0.016142</td>\n",
       "      <td>0.983858</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date  finbert_news_count  finbert_score_mean  finbert_active_score_mean  finbert_neg_share  finbert_neu_share  finbert_pos_share  finbert_active_neg_share  finbert_active_pos_share  \\\n",
       "0      A 2022-09-11                   1           -0.059356                  -0.396502           0.104527           0.850301           0.045172                  0.698251                  0.301749   \n",
       "1      A 2022-09-15                   1           -0.927253                  -0.947452           0.952966           0.021320           0.025714                  0.973726                  0.026274   \n",
       "2      A 2022-09-16                   2            0.056851                   0.610298           0.016570           0.910010           0.073420                  0.194851                  0.805149   \n",
       "3      A 2022-09-29                   1            0.018935                   0.283029           0.023983           0.933100           0.042917                  0.358485                  0.641515   \n",
       "4      A 2022-10-04                   1            0.585094                   0.967716           0.009760           0.395387           0.594854                  0.016142                  0.983858   \n",
       "\n",
       "   news_count  \n",
       "0           1  \n",
       "1           1  \n",
       "2           2  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_step(\"Сборка дневных агрегатов сентимента (8 моделей)\")\n",
    "\n",
    "def load_or_build_daily(model_tag: str, builder_fn, out_path: str) -> pd.DataFrame:\n",
    "    if SKIP_IF_DAILY_EXISTS and (not FORCE_REBUILD_DAILY) and os.path.exists(out_path):\n",
    "        try:\n",
    "            df = pd.read_parquet(out_path)\n",
    "            print(f\"[{model_tag}] найден итоговый файл -> {out_path} | строк: {len(df):,}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"[{model_tag}] не удалось прочитать {out_path} ({e}) -> заново\")\n",
    "\n",
    "    print(f\"[{model_tag}] считаю/продолжаю через чекпоинты...\")\n",
    "    df = builder_fn()\n",
    "    if not os.path.exists(out_path):\n",
    "        df.to_parquet(out_path, index=False)\n",
    "        print(f\"[{model_tag}] сохранён вручную: {out_path}\")\n",
    "    return df\n",
    "\n",
    "builders = {}\n",
    "\n",
    "# Lexicon models\n",
    "builders[\"vader\"] = lambda: build_vader_daily(band=0.02)\n",
    "builders[\"textblob\"] = lambda: build_textblob_daily(band=0.02)\n",
    "\n",
    "# Transformers\n",
    "builders[\"finbert\"] = lambda: build_hf_daily(\n",
    "    model_id=\"ProsusAI/finbert\",\n",
    "    prefix=\"finbert\",\n",
    "    batch_size=64,\n",
    "    max_length=192\n",
    ")\n",
    "\n",
    "builders[\"finbert_tone\"] = lambda: build_hf_daily(\n",
    "    model_id=\"yiyanghkust/finbert-tone\",\n",
    "    prefix=\"finbert_tone\",\n",
    "    batch_size=64,\n",
    "    max_length=192\n",
    ")\n",
    "\n",
    "builders[\"finroberta\"] = lambda: build_hf_daily(\n",
    "    model_id=\"soleimanian/financial-roberta-large-sentiment\",\n",
    "    prefix=\"finroberta\",\n",
    "    batch_size=32,\n",
    "    max_length=192\n",
    ")\n",
    "\n",
    "builders[\"distilroberta_finnews\"] = lambda: build_hf_daily(\n",
    "    model_id=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
    "    prefix=\"distilroberta_finnews\",\n",
    "    batch_size=64,\n",
    "    max_length=192\n",
    ")\n",
    "\n",
    "builders[\"deberta_finnews\"] = lambda: build_hf_daily(\n",
    "    model_id=\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",\n",
    "    prefix=\"deberta_finnews\",\n",
    "    batch_size=32,\n",
    "    max_length=192\n",
    ")\n",
    "\n",
    "builders[\"twroberta\"] = lambda: build_hf_daily(\n",
    "    model_id=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    prefix=\"twroberta\",\n",
    "    batch_size=64,\n",
    "    max_length=192\n",
    ")\n",
    "\n",
    "model_map = {}\n",
    "\n",
    "for tag, fn in builders.items():\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"daily_news_{tag}_2019_2023.parquet\")\n",
    "    model_map[tag] = load_or_build_daily(tag, fn, out_path)\n",
    "\n",
    "# news_count берём из finbert (как основной)\n",
    "model_map[\"finbert\"] = model_map[\"finbert\"].copy()\n",
    "fin_nc = \"finbert_news_count\"\n",
    "if fin_nc in model_map[\"finbert\"].columns:\n",
    "    model_map[\"finbert\"][\"news_count\"] = model_map[\"finbert\"][fin_nc]\n",
    "else:\n",
    "    raise ValueError(\"В finbert не найден finbert_news_count — проверь build_hf_daily/выходные колонки.\")\n",
    "\n",
    "display(model_map[\"finbert\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491353a8",
   "metadata": {},
   "source": [
    "## Блок 14: Выравнивание новостей к торговым дням\n",
    "\n",
    "Новости публикуются в календарные дни, а рынок не работает по выходным/праздникам\n",
    "\n",
    "Маппим `date` новости на **ближайший торговый день вперёд**\n",
    "\n",
    "Если новость вышла в выходной, эффект чаще проявляется на следующей торговой сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9abce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "Выравнивание моделей к торговым дням\n",
      "========================================================================================\n",
      "Строки после выравнивания:\n",
      "  - vader: 410,311\n",
      "  - textblob: 410,311\n",
      "  - finbert: 410,311\n",
      "  - finbert_tone: 410,311\n",
      "  - finroberta: 410,311\n",
      "  - distilroberta_finnews: 410,311\n",
      "  - deberta_finnews: 410,311\n",
      "  - twroberta: 410,311\n"
     ]
    }
   ],
   "source": [
    "def align_daily_to_trade_per_ticker(daily_df: pd.DataFrame, day_col: str = \"date\") -> pd.DataFrame:\n",
    "    a = daily_df.copy()\n",
    "    a[\"ticker\"] = a[\"ticker\"].astype(str).str.strip()\n",
    "    a[day_col] = pd.to_datetime(a[day_col], errors=\"coerce\").dt.normalize()\n",
    "    a = a.dropna(subset=[\"ticker\", day_col])\n",
    "    a = a.sort_values([\"ticker\", day_col], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    if \"news_count\" in a.columns:\n",
    "        count_col = \"news_count\"\n",
    "    else:\n",
    "        nc = [c for c in a.columns if c.endswith(\"_news_count\")]\n",
    "        count_col = nc[0] if nc else None\n",
    "\n",
    "    if count_col is None:\n",
    "        count_col = \"_tmp_news_count\"\n",
    "        a[count_col] = 1\n",
    "\n",
    "    out_parts = []\n",
    "    for tic, g in a.groupby(\"ticker\", sort=False):\n",
    "        tcal = trade.loc[trade[\"ticker\"] == tic, [\"trade_date\"]].sort_values(\"trade_date\")\n",
    "        if tcal.empty:\n",
    "            continue\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            g.sort_values(day_col, kind=\"mergesort\"),\n",
    "            tcal,\n",
    "            left_on=day_col,\n",
    "            right_on=\"trade_date\",\n",
    "            direction=\"forward\",\n",
    "            allow_exact_matches=True,\n",
    "            tolerance=pd.Timedelta(\"7D\"),\n",
    "        )\n",
    "\n",
    "        merged = merged.dropna(subset=[\"trade_date\"]).copy()\n",
    "        if merged.empty:\n",
    "            continue\n",
    "\n",
    "        merged = merged.drop(columns=[day_col]).rename(columns={\"trade_date\": \"date\"})\n",
    "        merged[\"date\"] = pd.to_datetime(merged[\"date\"]).dt.normalize()\n",
    "\n",
    "        num_cols = [\n",
    "            c for c in merged.columns\n",
    "            if c not in [\"ticker\", \"date\", count_col] and pd.api.types.is_numeric_dtype(merged[c])\n",
    "        ]\n",
    "\n",
    "        w = pd.to_numeric(merged[count_col], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "        tmp = merged[[\"ticker\", \"date\"] + num_cols].copy()\n",
    "        tmp[count_col] = w.values\n",
    "\n",
    "        do_not_weight = {count_col}\n",
    "        do_not_weight.update([c for c in num_cols if c.endswith(\"_news_count\")])\n",
    "        weighted_cols = [c for c in num_cols if c not in do_not_weight]\n",
    "\n",
    "        for c in weighted_cols:\n",
    "            tmp[c] = pd.to_numeric(tmp[c], errors=\"coerce\").astype(float) * tmp[count_col]\n",
    "\n",
    "        agg = tmp.groupby([\"ticker\", \"date\"], sort=False).sum(numeric_only=True)\n",
    "\n",
    "        denom = agg[count_col].replace(0, np.nan)\n",
    "        for c in weighted_cols:\n",
    "            agg[c] = agg[c] / denom\n",
    "\n",
    "        agg[count_col] = agg[count_col].fillna(0).round().astype(\"int32\")\n",
    "        out_parts.append(agg.reset_index())\n",
    "\n",
    "    out = pd.concat(out_parts, ignore_index=True) if out_parts else a.iloc[0:0].copy()\n",
    "    out = out.sort_values([\"ticker\", \"date\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    if \"_tmp_news_count\" in out.columns:\n",
    "        out = out.drop(columns=[\"_tmp_news_count\"])\n",
    "    return out\n",
    "\n",
    "print_step(\"Выравнивание моделей к торговым дням\")\n",
    "\n",
    "aligned = {}\n",
    "for tag, df in model_map.items():\n",
    "    aligned[tag] = align_daily_to_trade_per_ticker(df, day_col=\"date\")\n",
    "    dup = aligned[tag].duplicated([\"ticker\", \"date\"]).sum()\n",
    "    if dup:\n",
    "        raise ValueError(f\"{tag}: дубликаты (ticker,date) после выравнивания: {dup}\")\n",
    "\n",
    "print(\"Строки после выравнивания:\")\n",
    "for tag, df in aligned.items():\n",
    "    print(f\"  - {tag}: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbe741",
   "metadata": {},
   "source": [
    "## Блок 15: Сборка master‑датасета\n",
    "\n",
    "Берем доходности `ret` (включая excess и цели y1/y2/y3/y5), выровненные агрегаты сентимента из 8 моделей\n",
    "\n",
    "Создаем:\n",
    "- `news_n` = число новостей (по finbert)\n",
    "- `has_news_today` = индикатор дня с новостями\n",
    "\n",
    "В дни без новостей сентимент заполняем 0, чтобы можно было использовать все торговые дни в моделях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1da052cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "Сборка master\n",
      "========================================================================================\n",
      "MASTER: строк 2,755,903, тикеров 2193, колонок 76\n",
      "Период: 2019-01-02 — 2023-12-28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>ret_log</th>\n",
       "      <th>volume</th>\n",
       "      <th>price_col_used</th>\n",
       "      <th>mkt_ret_log</th>\n",
       "      <th>excess_ret_log</th>\n",
       "      <th>y1_ex</th>\n",
       "      <th>y2_ex</th>\n",
       "      <th>y3_ex</th>\n",
       "      <th>y5_ex</th>\n",
       "      <th>finbert_news_count</th>\n",
       "      <th>finbert_score_mean</th>\n",
       "      <th>finbert_active_score_mean</th>\n",
       "      <th>finbert_neg_share</th>\n",
       "      <th>finbert_neu_share</th>\n",
       "      <th>finbert_pos_share</th>\n",
       "      <th>finbert_active_neg_share</th>\n",
       "      <th>finbert_active_pos_share</th>\n",
       "      <th>news_count</th>\n",
       "      <th>vader_score_mean</th>\n",
       "      <th>vader_neg_share</th>\n",
       "      <th>vader_pos_share</th>\n",
       "      <th>vader_neu_share</th>\n",
       "      <th>vader_active_score_mean</th>\n",
       "      <th>vader_news_count</th>\n",
       "      <th>textblob_score_mean</th>\n",
       "      <th>textblob_neg_share</th>\n",
       "      <th>textblob_pos_share</th>\n",
       "      <th>textblob_neu_share</th>\n",
       "      <th>textblob_active_score_mean</th>\n",
       "      <th>textblob_news_count</th>\n",
       "      <th>finbert_tone_score_mean</th>\n",
       "      <th>finbert_tone_active_score_mean</th>\n",
       "      <th>finbert_tone_neg_share</th>\n",
       "      <th>finbert_tone_neu_share</th>\n",
       "      <th>finbert_tone_pos_share</th>\n",
       "      <th>finbert_tone_active_neg_share</th>\n",
       "      <th>finbert_tone_active_pos_share</th>\n",
       "      <th>finbert_tone_news_count</th>\n",
       "      <th>finroberta_score_mean</th>\n",
       "      <th>finroberta_active_score_mean</th>\n",
       "      <th>finroberta_neg_share</th>\n",
       "      <th>finroberta_neu_share</th>\n",
       "      <th>finroberta_pos_share</th>\n",
       "      <th>finroberta_active_neg_share</th>\n",
       "      <th>finroberta_active_pos_share</th>\n",
       "      <th>finroberta_news_count</th>\n",
       "      <th>distilroberta_finnews_score_mean</th>\n",
       "      <th>distilroberta_finnews_active_score_mean</th>\n",
       "      <th>distilroberta_finnews_neg_share</th>\n",
       "      <th>distilroberta_finnews_neu_share</th>\n",
       "      <th>distilroberta_finnews_pos_share</th>\n",
       "      <th>distilroberta_finnews_active_neg_share</th>\n",
       "      <th>distilroberta_finnews_active_pos_share</th>\n",
       "      <th>distilroberta_finnews_news_count</th>\n",
       "      <th>deberta_finnews_score_mean</th>\n",
       "      <th>deberta_finnews_active_score_mean</th>\n",
       "      <th>deberta_finnews_neg_share</th>\n",
       "      <th>deberta_finnews_neu_share</th>\n",
       "      <th>deberta_finnews_pos_share</th>\n",
       "      <th>deberta_finnews_active_neg_share</th>\n",
       "      <th>deberta_finnews_active_pos_share</th>\n",
       "      <th>deberta_finnews_news_count</th>\n",
       "      <th>twroberta_score_mean</th>\n",
       "      <th>twroberta_active_score_mean</th>\n",
       "      <th>twroberta_neg_share</th>\n",
       "      <th>twroberta_neu_share</th>\n",
       "      <th>twroberta_pos_share</th>\n",
       "      <th>twroberta_active_neg_share</th>\n",
       "      <th>twroberta_active_pos_share</th>\n",
       "      <th>twroberta_news_count</th>\n",
       "      <th>sector</th>\n",
       "      <th>news_n</th>\n",
       "      <th>has_news_today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>64.968681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2113300.0</td>\n",
       "      <td>adj close</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.013383</td>\n",
       "      <td>-0.012302</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.022114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>62.575249</td>\n",
       "      <td>-0.037536</td>\n",
       "      <td>5383900.0</td>\n",
       "      <td>adj close</td>\n",
       "      <td>-0.024152</td>\n",
       "      <td>-0.013383</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.014239</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.041318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>64.741203</td>\n",
       "      <td>0.034028</td>\n",
       "      <td>3123700.0</td>\n",
       "      <td>adj close</td>\n",
       "      <td>0.032947</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.018360</td>\n",
       "      <td>0.034416</td>\n",
       "      <td>0.046694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>66.115929</td>\n",
       "      <td>0.021012</td>\n",
       "      <td>3235100.0</td>\n",
       "      <td>adj close</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.021258</td>\n",
       "      <td>0.027079</td>\n",
       "      <td>0.030665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>67.085175</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>1578100.0</td>\n",
       "      <td>adj close</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.016056</td>\n",
       "      <td>0.021877</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>0.026179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date      price   ret_log     volume price_col_used  mkt_ret_log  excess_ret_log     y1_ex     y2_ex     y3_ex     y5_ex  finbert_news_count  finbert_score_mean  \\\n",
       "0      A 2019-01-02  64.968681       NaN  2113300.0      adj close          NaN             NaN -0.013383 -0.012302  0.000856  0.022114                 NaN                 0.0   \n",
       "1      A 2019-01-03  62.575249 -0.037536  5383900.0      adj close    -0.024152       -0.013383  0.001081  0.014239  0.019441  0.041318                 NaN                 0.0   \n",
       "2      A 2019-01-04  64.741203  0.034028  3123700.0      adj close     0.032947        0.001081  0.013158  0.018360  0.034416  0.046694                 NaN                 0.0   \n",
       "3      A 2019-01-07  66.115929  0.021012  3235100.0      adj close     0.007854        0.013158  0.005202  0.021258  0.027079  0.030665                 NaN                 0.0   \n",
       "4      A 2019-01-08  67.085175  0.014553  1578100.0      adj close     0.009351        0.005202  0.016056  0.021877  0.028334  0.026179                 NaN                 0.0   \n",
       "\n",
       "   finbert_active_score_mean  finbert_neg_share  finbert_neu_share  finbert_pos_share  finbert_active_neg_share  finbert_active_pos_share  news_count  vader_score_mean  vader_neg_share  \\\n",
       "0                        0.0                0.0                0.0                0.0                       0.0                       0.0         NaN               0.0              0.0   \n",
       "1                        0.0                0.0                0.0                0.0                       0.0                       0.0         NaN               0.0              0.0   \n",
       "2                        0.0                0.0                0.0                0.0                       0.0                       0.0         NaN               0.0              0.0   \n",
       "3                        0.0                0.0                0.0                0.0                       0.0                       0.0         NaN               0.0              0.0   \n",
       "4                        0.0                0.0                0.0                0.0                       0.0                       0.0         NaN               0.0              0.0   \n",
       "\n",
       "   vader_pos_share  vader_neu_share  vader_active_score_mean  vader_news_count  textblob_score_mean  textblob_neg_share  textblob_pos_share  textblob_neu_share  textblob_active_score_mean  \\\n",
       "0              0.0              0.0                      0.0               NaN                  0.0                 0.0                 0.0                 0.0                         0.0   \n",
       "1              0.0              0.0                      0.0               NaN                  0.0                 0.0                 0.0                 0.0                         0.0   \n",
       "2              0.0              0.0                      0.0               NaN                  0.0                 0.0                 0.0                 0.0                         0.0   \n",
       "3              0.0              0.0                      0.0               NaN                  0.0                 0.0                 0.0                 0.0                         0.0   \n",
       "4              0.0              0.0                      0.0               NaN                  0.0                 0.0                 0.0                 0.0                         0.0   \n",
       "\n",
       "   textblob_news_count  finbert_tone_score_mean  finbert_tone_active_score_mean  finbert_tone_neg_share  finbert_tone_neu_share  finbert_tone_pos_share  finbert_tone_active_neg_share  \\\n",
       "0                  NaN                      0.0                             0.0                     0.0                     0.0                     0.0                            0.0   \n",
       "1                  NaN                      0.0                             0.0                     0.0                     0.0                     0.0                            0.0   \n",
       "2                  NaN                      0.0                             0.0                     0.0                     0.0                     0.0                            0.0   \n",
       "3                  NaN                      0.0                             0.0                     0.0                     0.0                     0.0                            0.0   \n",
       "4                  NaN                      0.0                             0.0                     0.0                     0.0                     0.0                            0.0   \n",
       "\n",
       "   finbert_tone_active_pos_share  finbert_tone_news_count  finroberta_score_mean  finroberta_active_score_mean  finroberta_neg_share  finroberta_neu_share  finroberta_pos_share  \\\n",
       "0                            0.0                      NaN                    0.0                           0.0                   0.0                   0.0                   0.0   \n",
       "1                            0.0                      NaN                    0.0                           0.0                   0.0                   0.0                   0.0   \n",
       "2                            0.0                      NaN                    0.0                           0.0                   0.0                   0.0                   0.0   \n",
       "3                            0.0                      NaN                    0.0                           0.0                   0.0                   0.0                   0.0   \n",
       "4                            0.0                      NaN                    0.0                           0.0                   0.0                   0.0                   0.0   \n",
       "\n",
       "   finroberta_active_neg_share  finroberta_active_pos_share  finroberta_news_count  distilroberta_finnews_score_mean  distilroberta_finnews_active_score_mean  distilroberta_finnews_neg_share  \\\n",
       "0                          0.0                          0.0                    NaN                               0.0                                      0.0                              0.0   \n",
       "1                          0.0                          0.0                    NaN                               0.0                                      0.0                              0.0   \n",
       "2                          0.0                          0.0                    NaN                               0.0                                      0.0                              0.0   \n",
       "3                          0.0                          0.0                    NaN                               0.0                                      0.0                              0.0   \n",
       "4                          0.0                          0.0                    NaN                               0.0                                      0.0                              0.0   \n",
       "\n",
       "   distilroberta_finnews_neu_share  distilroberta_finnews_pos_share  distilroberta_finnews_active_neg_share  distilroberta_finnews_active_pos_share  distilroberta_finnews_news_count  \\\n",
       "0                              0.0                              0.0                                     0.0                                     0.0                               NaN   \n",
       "1                              0.0                              0.0                                     0.0                                     0.0                               NaN   \n",
       "2                              0.0                              0.0                                     0.0                                     0.0                               NaN   \n",
       "3                              0.0                              0.0                                     0.0                                     0.0                               NaN   \n",
       "4                              0.0                              0.0                                     0.0                                     0.0                               NaN   \n",
       "\n",
       "   deberta_finnews_score_mean  deberta_finnews_active_score_mean  deberta_finnews_neg_share  deberta_finnews_neu_share  deberta_finnews_pos_share  deberta_finnews_active_neg_share  \\\n",
       "0                         0.0                                0.0                        0.0                        0.0                        0.0                               0.0   \n",
       "1                         0.0                                0.0                        0.0                        0.0                        0.0                               0.0   \n",
       "2                         0.0                                0.0                        0.0                        0.0                        0.0                               0.0   \n",
       "3                         0.0                                0.0                        0.0                        0.0                        0.0                               0.0   \n",
       "4                         0.0                                0.0                        0.0                        0.0                        0.0                               0.0   \n",
       "\n",
       "   deberta_finnews_active_pos_share  deberta_finnews_news_count  twroberta_score_mean  twroberta_active_score_mean  twroberta_neg_share  twroberta_neu_share  twroberta_pos_share  \\\n",
       "0                               0.0                         NaN                   0.0                          0.0                  0.0                  0.0                  0.0   \n",
       "1                               0.0                         NaN                   0.0                          0.0                  0.0                  0.0                  0.0   \n",
       "2                               0.0                         NaN                   0.0                          0.0                  0.0                  0.0                  0.0   \n",
       "3                               0.0                         NaN                   0.0                          0.0                  0.0                  0.0                  0.0   \n",
       "4                               0.0                         NaN                   0.0                          0.0                  0.0                  0.0                  0.0   \n",
       "\n",
       "   twroberta_active_neg_share  twroberta_active_pos_share  twroberta_news_count      sector  news_n  has_news_today  \n",
       "0                         0.0                         0.0                   NaN  Healthcare       0               0  \n",
       "1                         0.0                         0.0                   NaN  Healthcare       0               0  \n",
       "2                         0.0                         0.0                   NaN  Healthcare       0               0  \n",
       "3                         0.0                         0.0                   NaN  Healthcare       0               0  \n",
       "4                         0.0                         0.0                   NaN  Healthcare       0               0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_step(\"Сборка master\")\n",
    "\n",
    "master = ret.copy()\n",
    "\n",
    "master = master.merge(aligned[\"finbert\"], on=[\"ticker\", \"date\"], how=\"left\")\n",
    "\n",
    "for tag in [k for k in aligned.keys() if k != \"finbert\"]:\n",
    "    cols = [c for c in aligned[tag].columns if c not in [\"ticker\", \"date\"]]\n",
    "    master = master.merge(aligned[tag][[\"ticker\", \"date\"] + cols], on=[\"ticker\", \"date\"], how=\"left\")\n",
    "\n",
    "if \"sector\" in companies.columns:\n",
    "    master = master.merge(companies[[\"ticker\", \"sector\"]], on=\"ticker\", how=\"left\")\n",
    "\n",
    "master[\"news_n\"] = pd.to_numeric(master.get(\"news_count\"), errors=\"coerce\").fillna(0).astype(\"int32\")\n",
    "master[\"has_news_today\"] = (master[\"news_n\"] > 0).astype(\"int8\")\n",
    "\n",
    "master = master.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "sent_cols = [c for c in master.columns if any(\n",
    "    c.endswith(suf) for suf in [\n",
    "        \"_score_mean\", \"_active_score_mean\",\n",
    "        \"_neg_share\", \"_neu_share\", \"_pos_share\",\n",
    "        \"_active_neg_share\", \"_active_pos_share\"\n",
    "    ]\n",
    ")]\n",
    "\n",
    "mask_no_news = master[\"has_news_today\"] == 0\n",
    "master.loc[mask_no_news, sent_cols] = master.loc[mask_no_news, sent_cols].fillna(0.0)\n",
    "\n",
    "master = master.sort_values([\"ticker\", \"date\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "print(f\"MASTER: строк {len(master):,}, тикеров {master['ticker'].nunique()}, колонок {master.shape[1]}\")\n",
    "print(f\"Период: {master['date'].min().date()} — {master['date'].max().date()}\")\n",
    "\n",
    "display(master.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36727a",
   "metadata": {},
   "source": [
    "## Блок 16: Сохранение и валидация результатов\n",
    "\n",
    "`outputs_final/returns_sentiment_enhanced.parquet`\n",
    "\n",
    "Контрольные проверки:\n",
    "- нет ли дубликатов `(ticker, date)`\n",
    "- присутствуют обязательные колонки\n",
    "- доля дней с новостями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0519c1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "Проверки и сохранение\n",
      "========================================================================================\n",
      "Дубликаты (ticker,date): 0\n",
      "Все обязательные колонки присутствуют\n",
      "Доля дней с новостями (has_news_today): 14.9%\n",
      "Master сохранен: outputs_final/returns_sentiment_enhanced.parquet\n"
     ]
    }
   ],
   "source": [
    "print_step(\"Проверки и сохранение\")\n",
    "\n",
    "dup_master = master.duplicated(subset=[\"ticker\", \"date\"]).sum()\n",
    "print(\"Дубликаты (ticker,date):\", int(dup_master))\n",
    "if dup_master:\n",
    "    raise ValueError(\"Обнаружены дубликаты в master — это нужно исправить до моделирования.\")\n",
    "\n",
    "required = [\n",
    "    \"ticker\", \"date\", \"ret_log\", \"mkt_ret_log\", \"excess_ret_log\",\n",
    "    \"y1_ex\", \"y2_ex\", \"y3_ex\", \"y5_ex\",\n",
    "    \"has_news_today\", \"news_n\",\n",
    "    \"finbert_score_mean\", \"vader_score_mean\", \"textblob_score_mean\",\n",
    "]\n",
    "missing = [c for c in required if c not in master.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Не хватает обязательных колонок: {missing}\")\n",
    "\n",
    "print(\"Все обязательные колонки присутствуют\")\n",
    "print(\"Доля дней с новостями (has_news_today):\", f\"{master['has_news_today'].mean()*100:.1f}%\")\n",
    "\n",
    "MASTER_PATH = os.path.join(OUTPUT_DIR, \"returns_sentiment_enhanced.parquet\")\n",
    "master.to_parquet(MASTER_PATH, index=False)\n",
    "print(f\"Master сохранен: {MASTER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4579c2e",
   "metadata": {},
   "source": [
    "## Блок 17: 20 примеров (новости + цена + доходности)\n",
    "\n",
    "Берём 10 самых негативных и 10 самых позитивных дней по выбранной колонке сентимента и выводим примеры текстов новостей (до 3 на кейс)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f80b6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тикеров в примерах: 19\n",
      "Окно дат: 2019-11-19 — 2023-12-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read texts: all_external: 66it [00:44,  1.49it/s]\n",
      "Read texts: nasdaq: 78it [02:22,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено строк новостей (после фильтров): 11982\n",
      "Доля кейсов с найденными текстами: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>ret_log</th>\n",
       "      <th>excess_ret_log</th>\n",
       "      <th>y1_ex</th>\n",
       "      <th>y3_ex</th>\n",
       "      <th>news_n</th>\n",
       "      <th>finbert_score_mean</th>\n",
       "      <th>example_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RCUS</td>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>31.469999</td>\n",
       "      <td>0.021520</td>\n",
       "      <td>0.014786</td>\n",
       "      <td>-0.055839</td>\n",
       "      <td>-0.058613</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.969681</td>\n",
       "      <td>Gilead earnings hurt by legal settlement, other charges. By Deena Beasley Feb 1 (Reuters) - Gilead Sciences Inc GILD.O on Tuesday posted lower-than-expected fourth-quarter earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIV</td>\n",
       "      <td>2020-05-19</td>\n",
       "      <td>7.789398</td>\n",
       "      <td>-0.026766</td>\n",
       "      <td>-0.016442</td>\n",
       "      <td>-0.000943</td>\n",
       "      <td>0.023319</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.969272</td>\n",
       "      <td>TIM shares drop after core profit guidance omitted. By Elvira Pollina MILAN, May 19 (Reuters) - Shares in Telecom Italia (TIM) TLIT.MI fell sharply on Tuesday as Italy's bigges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMC</td>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>55.986725</td>\n",
       "      <td>0.009413</td>\n",
       "      <td>0.020099</td>\n",
       "      <td>-0.005069</td>\n",
       "      <td>-0.001542</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.969266</td>\n",
       "      <td>LyondellBasell's (LYB) Q4 Earnings Beat, Sales Lag Estimates. LyondellBasell Industries N.V. LYB recorded earnings of $353 million or $1.07 per share in the fourth quarter of 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRK</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>11.336234</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>-0.015174</td>\n",
       "      <td>-0.010678</td>\n",
       "      <td>-0.045819</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.969080</td>\n",
       "      <td>US drillers cut oil and gas rigs for ninth week in a row -Baker Hughes. Adds Haynesville and Williston basin counts June 30 (Reuters) - U.S. energy firms this week cut the numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BKR</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>31.254986</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>-0.003791</td>\n",
       "      <td>-0.001466</td>\n",
       "      <td>0.007883</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.969080</td>\n",
       "      <td>US drillers cut oil and gas rigs for ninth week in a row -Baker Hughes. Adds Haynesville and Williston basin counts June 30 (Reuters) - U.S. energy firms this week cut the numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HPE</td>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>15.708621</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.011506</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>-0.011249</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.968973</td>\n",
       "      <td>Global PC shipments slide in Q1, Apple takes biggest hit - IDC. Adds graphic April 10 (Reuters) - Global shipments of personal computers (PCs) fell by 29% in the first quarter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CRK</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>10.231929</td>\n",
       "      <td>-0.019859</td>\n",
       "      <td>-0.016447</td>\n",
       "      <td>-0.019945</td>\n",
       "      <td>-0.011578</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.968785</td>\n",
       "      <td>US drillers cut oil and gas rigs for seventh week in a row - Baker Hughes. Adds rig reductions in Marcellus and Permian basins June 16 (Reuters) - U.S. energy firms this week c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>REZI</td>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>15.830000</td>\n",
       "      <td>-0.389118</td>\n",
       "      <td>-0.363702</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.024350</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.968765</td>\n",
       "      <td>Consumer Sector Update for 11/02/2022: TUP, REZI, LL. Consumer stocks were broadly lower in Wednesday trading, with the SPDR Consumer Staples Select Sector ETF (XLP) dropping 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AIG</td>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>50.340004</td>\n",
       "      <td>-0.017209</td>\n",
       "      <td>-0.008788</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>-0.006030</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.968749</td>\n",
       "      <td>Blackstone's earnings fall 16% on sharp drop in asset sales. By Chibuike Oguh NEW YORK, Oct 20 (Reuters) - Blackstone Inc BX.N, the world's largest alternative asset manager, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EQNR</td>\n",
       "      <td>2023-10-23</td>\n",
       "      <td>32.608925</td>\n",
       "      <td>-0.014517</td>\n",
       "      <td>-0.012782</td>\n",
       "      <td>-0.021939</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.968742</td>\n",
       "      <td>Norwegian Sept gas output lags forecasts, hits 4-year low on outages. Adds comparisons, graphic, detail in paragraphs 4-7 OSLO, Oct 23 (Reuters) - Norway's natural gas output f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ALGT</td>\n",
       "      <td>2023-02-24</td>\n",
       "      <td>99.355186</td>\n",
       "      <td>-0.006616</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>-0.001322</td>\n",
       "      <td>0.025998</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943070</td>\n",
       "      <td>Allegiant (ALGT) Shares Rise 20.5% in a Month: Here's Why. Allegiant Travel Company ALGT is being aided by the upbeat air travel demand scenario in the United States. Owing to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FLO</td>\n",
       "      <td>2023-10-02</td>\n",
       "      <td>21.637939</td>\n",
       "      <td>-0.013618</td>\n",
       "      <td>-0.013220</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>-0.005356</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943088</td>\n",
       "      <td>Pilgrim's Pride (PPC) is on Growth Trajectory Amid Cost Woes. Pilgrim’s Pride Corporation's PPC ongoing focus on operational excellence, manufacturing network optimization and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GLW</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>33.808277</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>-0.004733</td>\n",
       "      <td>-0.007584</td>\n",
       "      <td>-0.017131</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943098</td>\n",
       "      <td>Will Top-Line Expansion Boost Nokia's (NOK) Q1 Earnings?. Nokia Corporation NOK is scheduled to report first-quarter 2023 results on Apr 20. In the last reported quarter, the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ALG</td>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>171.433487</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>-0.005618</td>\n",
       "      <td>0.031706</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943120</td>\n",
       "      <td>Applied Industrial (AIT) Up 22.7% YTD: Will the Trend Last?. Shares of Applied Industrial Technologies, Inc. AIT have rallied 22.7% in the year-to-date period, outperforming th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NTES</td>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>113.480003</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>-0.000797</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.099960</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943343</td>\n",
       "      <td>Nutanix (NTNX) Q1 Earnings Top Estimates, Revenues Rise Y/Y. Nutanix NTNX stock jumped more than 6% during Wednesday’s extended trading session after the company reported bette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BBY</td>\n",
       "      <td>2023-11-03</td>\n",
       "      <td>66.354553</td>\n",
       "      <td>0.012581</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>-0.005727</td>\n",
       "      <td>-0.055451</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943375</td>\n",
       "      <td>Expedia (EXPE) Q3 Earnings Beat Estimates, Revenues Rise Y/Y. Expedia Group, Inc. EXPE delivered third-quarter 2023 adjusted earnings of $5.41 per share, up 33% from the year-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GIL</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>34.683102</td>\n",
       "      <td>-0.017826</td>\n",
       "      <td>-0.007776</td>\n",
       "      <td>-0.006688</td>\n",
       "      <td>-0.026346</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943537</td>\n",
       "      <td>G-III Apparel's (GIII) Digital &amp; Brand Progress Look Promising. G-III Apparel Group, Ltd. GIII appears impressive on the back of robust digital gains and strength in its global...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CARG</td>\n",
       "      <td>2019-11-26</td>\n",
       "      <td>39.709999</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.003944</td>\n",
       "      <td>-0.003598</td>\n",
       "      <td>2</td>\n",
       "      <td>0.944178</td>\n",
       "      <td>Why CarGurus' Stock Is Up 30% Over The Last Month. CarGurus (NASDAQ: CARG), an online automobile market place that connects buyers and sellers of new and used cars, has seen it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ACGL</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>48.689999</td>\n",
       "      <td>0.049684</td>\n",
       "      <td>0.019168</td>\n",
       "      <td>-0.012775</td>\n",
       "      <td>-0.023041</td>\n",
       "      <td>2</td>\n",
       "      <td>0.944345</td>\n",
       "      <td>Markel (MKL) Banks on Solid Segmental Growth Amid Cost Woes. Markel Corporation MKL has been gaining momentum on the back of new business volume, favorable rates, higher retent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DGX</td>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>141.315613</td>\n",
       "      <td>-0.031805</td>\n",
       "      <td>-0.011541</td>\n",
       "      <td>0.002771</td>\n",
       "      <td>-0.018508</td>\n",
       "      <td>2</td>\n",
       "      <td>0.944814</td>\n",
       "      <td>Quest Diagnostics (DGX) Base Business Growth Robust, Volume Up. Quest Diagnostics DGX has been focusing on areas with high potential. Positive demography and cost reduction ini...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker       date       price   ret_log  excess_ret_log     y1_ex     y3_ex  news_n  finbert_score_mean  \\\n",
       "0    RCUS 2022-02-01   31.469999  0.021520        0.014786 -0.055839 -0.058613       2           -0.969681   \n",
       "1     VIV 2020-05-19    7.789398 -0.026766       -0.016442 -0.000943  0.023319       2           -0.969272   \n",
       "2     CMC 2023-02-03   55.986725  0.009413        0.020099 -0.005069 -0.001542       2           -0.969266   \n",
       "3     CRK 2023-06-30   11.336234 -0.003442       -0.015174 -0.010678 -0.045819       2           -0.969080   \n",
       "4     BKR 2023-06-30   31.254986  0.007940       -0.003791 -0.001466  0.007883       2           -0.969080   \n",
       "5     HPE 2023-04-10   15.708621  0.012531        0.011506 -0.000891 -0.011249       2           -0.968973   \n",
       "6     CRK 2023-06-16   10.231929 -0.019859       -0.016447 -0.019945 -0.011578       2           -0.968785   \n",
       "7    REZI 2022-11-02   15.830000 -0.389118       -0.363702  0.002104  0.024350       2           -0.968765   \n",
       "8     AIG 2022-10-20   50.340004 -0.017209       -0.008788  0.002632 -0.006030       2           -0.968749   \n",
       "9    EQNR 2023-10-23   32.608925 -0.014517       -0.012782 -0.021939  0.002443       2           -0.968742   \n",
       "19   ALGT 2023-02-24   99.355186 -0.006616        0.004124 -0.001322  0.025998       2            0.943070   \n",
       "18    FLO 2023-10-02   21.637939 -0.013618       -0.013220  0.004755 -0.005356       2            0.943088   \n",
       "17    GLW 2023-04-17   33.808277 -0.001151       -0.004733 -0.007584 -0.017131       2            0.943098   \n",
       "16    ALG 2023-08-31  171.433487 -0.007082       -0.005618  0.031706  0.007405       2            0.943120   \n",
       "15   NTES 2023-11-30  113.480003  0.003133       -0.000797 -0.042697 -0.099960       2            0.943343   \n",
       "14    BBY 2023-11-03   66.354553  0.012581        0.003499 -0.005727 -0.055451       2            0.943375   \n",
       "13    GIL 2022-04-06   34.683102 -0.017826       -0.007776 -0.006688 -0.026346       2            0.943537   \n",
       "12   CARG 2019-11-26   39.709999  0.001764       -0.000498 -0.003944 -0.003598       2            0.944178   \n",
       "11   ACGL 2022-10-04   48.689999  0.049684        0.019168 -0.012775 -0.023041       2            0.944345   \n",
       "10    DGX 2023-02-21  141.315613 -0.031805       -0.011541  0.002771 -0.018508       2            0.944814   \n",
       "\n",
       "                                                                                                                                                                           example_news  \n",
       "0   Gilead earnings hurt by legal settlement, other charges. By Deena Beasley Feb 1 (Reuters) - Gilead Sciences Inc GILD.O on Tuesday posted lower-than-expected fourth-quarter earn...  \n",
       "1   TIM shares drop after core profit guidance omitted. By Elvira Pollina MILAN, May 19 (Reuters) - Shares in Telecom Italia (TIM) TLIT.MI fell sharply on Tuesday as Italy's bigges...  \n",
       "2   LyondellBasell's (LYB) Q4 Earnings Beat, Sales Lag Estimates. LyondellBasell Industries N.V. LYB recorded earnings of $353 million or $1.07 per share in the fourth quarter of 2...  \n",
       "3   US drillers cut oil and gas rigs for ninth week in a row -Baker Hughes. Adds Haynesville and Williston basin counts June 30 (Reuters) - U.S. energy firms this week cut the numb...  \n",
       "4   US drillers cut oil and gas rigs for ninth week in a row -Baker Hughes. Adds Haynesville and Williston basin counts June 30 (Reuters) - U.S. energy firms this week cut the numb...  \n",
       "5   Global PC shipments slide in Q1, Apple takes biggest hit - IDC. Adds graphic April 10 (Reuters) - Global shipments of personal computers (PCs) fell by 29% in the first quarter ...  \n",
       "6   US drillers cut oil and gas rigs for seventh week in a row - Baker Hughes. Adds rig reductions in Marcellus and Permian basins June 16 (Reuters) - U.S. energy firms this week c...  \n",
       "7   Consumer Sector Update for 11/02/2022: TUP, REZI, LL. Consumer stocks were broadly lower in Wednesday trading, with the SPDR Consumer Staples Select Sector ETF (XLP) dropping 0...  \n",
       "8   Blackstone's earnings fall 16% on sharp drop in asset sales. By Chibuike Oguh NEW YORK, Oct 20 (Reuters) - Blackstone Inc BX.N, the world's largest alternative asset manager, s...  \n",
       "9   Norwegian Sept gas output lags forecasts, hits 4-year low on outages. Adds comparisons, graphic, detail in paragraphs 4-7 OSLO, Oct 23 (Reuters) - Norway's natural gas output f...  \n",
       "19  Allegiant (ALGT) Shares Rise 20.5% in a Month: Here's Why. Allegiant Travel Company ALGT is being aided by the upbeat air travel demand scenario in the United States. Owing to ...  \n",
       "18  Pilgrim's Pride (PPC) is on Growth Trajectory Amid Cost Woes. Pilgrim’s Pride Corporation's PPC ongoing focus on operational excellence, manufacturing network optimization and ...  \n",
       "17  Will Top-Line Expansion Boost Nokia's (NOK) Q1 Earnings?. Nokia Corporation NOK is scheduled to report first-quarter 2023 results on Apr 20. In the last reported quarter, the c...  \n",
       "16  Applied Industrial (AIT) Up 22.7% YTD: Will the Trend Last?. Shares of Applied Industrial Technologies, Inc. AIT have rallied 22.7% in the year-to-date period, outperforming th...  \n",
       "15  Nutanix (NTNX) Q1 Earnings Top Estimates, Revenues Rise Y/Y. Nutanix NTNX stock jumped more than 6% during Wednesday’s extended trading session after the company reported bette...  \n",
       "14  Expedia (EXPE) Q3 Earnings Beat Estimates, Revenues Rise Y/Y. Expedia Group, Inc. EXPE delivered third-quarter 2023 adjusted earnings of $5.41 per share, up 33% from the year-a...  \n",
       "13  G-III Apparel's (GIII) Digital & Brand Progress Look Promising. G-III Apparel Group, Ltd. GIII appears impressive on the back of robust digital gains and strength in its global...  \n",
       "12  Why CarGurus' Stock Is Up 30% Over The Last Month. CarGurus (NASDAQ: CARG), an online automobile market place that connects buyers and sellers of new and used cars, has seen it...  \n",
       "11  Markel (MKL) Banks on Solid Segmental Growth Amid Cost Woes. Markel Corporation MKL has been gaining momentum on the back of new business volume, favorable rates, higher retent...  \n",
       "10  Quest Diagnostics (DGX) Base Business Growth Robust, Volume Up. Quest Diagnostics DGX has been focusing on areas with high potential. Positive demography and cost reduction ini...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SAMPLE_N = 20\n",
    "MIN_NEWS = 2\n",
    "SENT_COL = \"finbert_score_mean\"\n",
    "MAX_HEADLINES_PER_CASE = 3\n",
    "\n",
    "cand = master.loc[\n",
    "    (master[\"has_news_today\"] == 1) &\n",
    "    (master[\"news_n\"] >= MIN_NEWS) &\n",
    "    (master[SENT_COL].notna())\n",
    "].copy()\n",
    "\n",
    "k = SAMPLE_N // 2\n",
    "neg_cases = cand.sort_values(SENT_COL, ascending=True).head(k)\n",
    "pos_cases = cand.sort_values(SENT_COL, ascending=False).head(k)\n",
    "cases = pd.concat([neg_cases, pos_cases], ignore_index=True)\n",
    "\n",
    "cases = cases[[\n",
    "    \"ticker\", \"date\", \"price\", \"ret_log\", \"excess_ret_log\",\n",
    "    \"y1_ex\", \"y3_ex\", \"news_n\", SENT_COL\n",
    "]].copy()\n",
    "\n",
    "tickers_need = set(cases[\"ticker\"].unique())\n",
    "date_min = (cases[\"date\"].min() - pd.Timedelta(days=7)).normalize()\n",
    "date_max = (cases[\"date\"].max() + pd.Timedelta(days=1)).normalize()\n",
    "\n",
    "print(\"Тикеров в примерах:\", len(tickers_need))\n",
    "print(\"Окно дат:\", date_min.date(), \"—\", date_max.date())\n",
    "\n",
    "def load_news_texts_subset_per_source(source_tag: str, file_path: str,\n",
    "                                      tickers_set: set, dmin: pd.Timestamp, dmax: pd.Timestamp,\n",
    "                                      variant: str,\n",
    "                                      skip_keys_sorted: Optional[np.ndarray]) -> pd.DataFrame:\n",
    "    out = []\n",
    "    reader = pd.read_csv(file_path, chunksize=CHUNK_SIZE, low_memory=False)\n",
    "\n",
    "    for raw in tqdm(reader, desc=f\"Read texts: {source_tag}\"):\n",
    "        ch = _prep_news_chunk_base(raw, source_tag)\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        ch = ch[ch[\"ticker\"].isin(tickers_set)]\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        ch = ch[(ch[\"day\"] >= dmin) & (ch[\"day\"] <= dmax)]\n",
    "        if ch.empty:\n",
    "            continue\n",
    "\n",
    "        if (skip_keys_sorted is not None) and len(skip_keys_sorted) > 0:\n",
    "            kh = _hash_url_ticker(ch)\n",
    "            dup_mask = _is_dup_by_keys(skip_keys_sorted, kh)\n",
    "            if dup_mask.any():\n",
    "                ch = ch.loc[~dup_mask].reset_index(drop=True)\n",
    "                if ch.empty:\n",
    "                    continue\n",
    "\n",
    "        text = build_text_variant(ch, variant)\n",
    "        m = (text != \"\")\n",
    "        ch = ch.loc[m, [\"ticker\",\"day\",\"url\",\"source\"]].copy()\n",
    "        ch[\"text\"] = text.loc[m].astype(str)\n",
    "\n",
    "        out.append(ch)\n",
    "\n",
    "    if not out:\n",
    "        return pd.DataFrame(columns=[\"ticker\",\"day\",\"url\",\"source\",\"text\"])\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "texts_parts = []\n",
    "for tag, fp in NEWS_SOURCES.items():\n",
    "    var = selected_variants[tag]\n",
    "    skip = None if tag == preferred_source else preferred_keys_sorted\n",
    "    part = load_news_texts_subset_per_source(tag, fp, tickers_need, date_min, date_max, var, skip)\n",
    "    texts_parts.append(part)\n",
    "\n",
    "news_subset = pd.concat(texts_parts, ignore_index=True) if texts_parts else pd.DataFrame()\n",
    "print(\"Найдено строк новостей (после фильтров):\", len(news_subset))\n",
    "\n",
    "trade_small = trade[trade[\"ticker\"].isin(tickers_need)].copy()\n",
    "trade_small = trade_small.sort_values([\"ticker\", \"trade_date\"], kind=\"mergesort\")\n",
    "\n",
    "mapped_parts = []\n",
    "for tic, g in news_subset.groupby(\"ticker\", sort=False):\n",
    "    tcal = trade_small.loc[trade_small[\"ticker\"] == tic, [\"trade_date\"]].sort_values(\"trade_date\")\n",
    "    if tcal.empty:\n",
    "        continue\n",
    "\n",
    "    merged = pd.merge_asof(\n",
    "        g.sort_values(\"day\", kind=\"mergesort\"),\n",
    "        tcal,\n",
    "        left_on=\"day\",\n",
    "        right_on=\"trade_date\",\n",
    "        direction=\"forward\",\n",
    "        allow_exact_matches=True,\n",
    "        tolerance=pd.Timedelta(\"7D\"),\n",
    "    )\n",
    "    merged = merged.dropna(subset=[\"trade_date\"]).copy()\n",
    "    if merged.empty:\n",
    "        continue\n",
    "\n",
    "    merged[\"date\"] = pd.to_datetime(merged[\"trade_date\"]).dt.normalize()\n",
    "    mapped_parts.append(merged[[\"ticker\", \"date\", \"text\", \"source\"]])\n",
    "\n",
    "mapped = pd.concat(mapped_parts, ignore_index=True) if mapped_parts else pd.DataFrame(columns=[\"ticker\",\"date\",\"text\",\"source\"])\n",
    "\n",
    "def pack_texts(s):\n",
    "    texts = s.tolist()[:MAX_HEADLINES_PER_CASE]\n",
    "    texts = [(t[:220] + (\"…\" if len(t) > 220 else \"\")) for t in texts]\n",
    "    return \" | \".join(texts)\n",
    "\n",
    "examples = (\n",
    "    mapped.groupby([\"ticker\", \"date\"], sort=False)[\"text\"]\n",
    "    .apply(pack_texts)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"text\": \"example_news\"})\n",
    ")\n",
    "\n",
    "cases = cases.merge(examples, on=[\"ticker\",\"date\"], how=\"left\")\n",
    "\n",
    "print(\"Доля кейсов с найденными текстами:\", f\"{cases['example_news'].notna().mean()*100:.1f}%\")\n",
    "display(cases.sort_values(SENT_COL))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
